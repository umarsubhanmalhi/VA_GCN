{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_name=\"Men\"\n",
    "subcat_name=\"Men_also_bought\"\n",
    "cat_rel=\"also_bought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TODO: clean unused code\n",
    "\n",
    "class DataLoaderAmazon(object):\n",
    "    \"\"\"\n",
    "    Load amazon data.\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_rel=cat_rel):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            normalize: normalize the features or not\n",
    "            cat_rel: category and type of relation used\n",
    "        \"\"\"\n",
    "        super(DataLoaderAmazon, self).__init__()\n",
    "        self.cat_rel = cat_rel\n",
    "\n",
    "        self.path_dataset = 'D:/School/FARS/Dataset/'+cat_name+'/'+cat_rel+'/'\n",
    "        assert os.path.exists(self.path_dataset)\n",
    "\n",
    "        print('initializing dataloader...')\n",
    "        self.init_dataset()\n",
    "\n",
    "    def init_dataset(self):\n",
    "        path_dataset = self.path_dataset\n",
    "        adj_file = path_dataset + 'adj.npz'\n",
    "        feats_file = path_dataset + 'deep_feats.npy'\n",
    "        np.random.seed(1234)\n",
    "\n",
    "        self.adj = sp.load_npz(adj_file).astype(np.int32)\n",
    "        node_features = np.load(feats_file)\n",
    "        self.features = node_features\n",
    "\n",
    "        # get lower tiangle of the adj matrix to avoid duplicate edges\n",
    "        self.lower_adj = sp.tril(self.adj).tocsr()\n",
    "\n",
    "        # get positive edges and split them into train, val and test\n",
    "        pos_r_idx, pos_c_idx = self.lower_adj.nonzero()\n",
    "        pos_labels = np.array(self.lower_adj[pos_r_idx, pos_c_idx]).squeeze()\n",
    "\n",
    "        n_pos = pos_labels.shape[0] # number of positive edges\n",
    "        perm = list(range(n_pos))\n",
    "        np.random.shuffle(perm)\n",
    "        pos_labels, pos_r_idx, pos_c_idx = pos_labels[perm], pos_r_idx[perm], pos_c_idx[perm]\n",
    "        n_train = int(n_pos*0.65)\n",
    "        n_val = int(n_pos*0.17)\n",
    "\n",
    "        self.train_pos_labels, self.train_pos_r_idx, self.train_pos_c_idx = pos_labels[:n_train], pos_r_idx[:n_train], pos_c_idx[:n_train]\n",
    "        self.val_pos_labels, self.val_pos_r_idx, self.val_pos_c_idx = pos_labels[n_train:n_train + n_val], pos_r_idx[n_train:n_train + n_val], pos_c_idx[n_train:n_train + n_val]\n",
    "        self.test_pos_labels, self.test_pos_r_idx, self.test_pos_c_idx = pos_labels[n_train + n_val:], pos_r_idx[n_train + n_val:], pos_c_idx[n_train + n_val:]\n",
    "\n",
    "    def get_phase(self, phase):\n",
    "        print('get phase: {}'.format(phase))\n",
    "        assert phase in ['train', 'valid', 'test']\n",
    "\n",
    "        lower_adj = self.lower_adj\n",
    "\n",
    "        # get the positive edges\n",
    "\n",
    "        if phase == 'train':\n",
    "            pos_labels, pos_r_idx, pos_c_idx = self.train_pos_labels, self.train_pos_r_idx, self.train_pos_c_idx\n",
    "        elif phase == 'valid':\n",
    "            pos_labels, pos_r_idx, pos_c_idx = self.val_pos_labels, self.val_pos_r_idx, self.val_pos_c_idx\n",
    "        elif phase == 'test':\n",
    "            pos_labels, pos_r_idx, pos_c_idx = self.test_pos_labels, self.test_pos_r_idx, self.test_pos_c_idx\n",
    "\n",
    "        # build adj matrix\n",
    "        full_adj = sp.csr_matrix((\n",
    "                    np.hstack([pos_labels, pos_labels]),\n",
    "                    (np.hstack([pos_r_idx, pos_c_idx]), np.hstack([pos_c_idx, pos_r_idx]))\n",
    "                ),\n",
    "                shape=(lower_adj.shape[0], lower_adj.shape[0])\n",
    "            )\n",
    "        setattr(self, 'full_{}_adj'.format(phase), full_adj)\n",
    "\n",
    "        # split the positive edges into the ones used for evaluation and the ones used as message passing\n",
    "        n_pos = pos_labels.shape[0] # number of positive edges\n",
    "        n_eval = int(n_pos/2)\n",
    "        mp_pos_labels, mp_pos_r_idx, mp_pos_c_idx = pos_labels[n_eval:], pos_r_idx[n_eval:], pos_c_idx[n_eval:]\n",
    "        # this are the positive examples that will be used to compute the loss function\n",
    "        eval_pos_labels, eval_pos_r_idx, eval_pos_c_idx = pos_labels[:n_eval], pos_r_idx[:n_eval], pos_c_idx[:n_eval]\n",
    "\n",
    "        # get the negative edges\n",
    "\n",
    "        print('Sampling negative edges...')\n",
    "        before = time.time()\n",
    "        n_train_neg = eval_pos_labels.shape[0] # set the number of negative training edges that will be needed to sample at each iter\n",
    "        neg_labels = np.zeros((n_train_neg))\n",
    "        # get the possible indexes to be sampled (basically all indexes if there aren't restrictions)\n",
    "        poss_nodes = np.arange(lower_adj.shape[0])\n",
    "\n",
    "        neg_r_idx = np.zeros((n_train_neg))\n",
    "        neg_c_idx = np.zeros((n_train_neg))\n",
    "\n",
    "        for i in range(n_train_neg):\n",
    "            r_idx, c_idx = self.get_negative_training_edge(poss_nodes, poss_nodes.shape[0], lower_adj)\n",
    "            neg_r_idx[i] = r_idx\n",
    "            neg_c_idx[i] = c_idx\n",
    "        print('Sampling done, time elapsed: {}'.format(time.time() - before))\n",
    "\n",
    "        # build adj matrix\n",
    "        adj = sp.csr_matrix((\n",
    "                    np.hstack([mp_pos_labels, mp_pos_labels]),\n",
    "                    (np.hstack([mp_pos_r_idx, mp_pos_c_idx]), np.hstack([mp_pos_c_idx, mp_pos_r_idx]))\n",
    "                ),\n",
    "                shape=(lower_adj.shape[0], lower_adj.shape[0])\n",
    "            )\n",
    "        # remove the labels of the negative edges which are 0\n",
    "        adj.eliminate_zeros()\n",
    "\n",
    "        labels = np.append(eval_pos_labels, neg_labels)\n",
    "        r_idx = np.append(eval_pos_r_idx, neg_r_idx)\n",
    "        c_idx = np.append(eval_pos_c_idx, neg_c_idx)\n",
    "\n",
    "        return self.features, adj, labels, r_idx, c_idx\n",
    "\n",
    "    def normalize_features(self, feats, get_moments=False, mean=None, std=None):\n",
    "        reuse_mean = mean is not None and std is not None\n",
    "        if feats.shape[1] == 256: # image features\n",
    "            if reuse_mean:\n",
    "                mean_feats = mean\n",
    "                std_feats = std\n",
    "            else:\n",
    "                mean_feats = feats.mean(axis=0)\n",
    "                std_feats = feats.std(axis=0)\n",
    "\n",
    "            # normalize\n",
    "            feats = (feats - mean_feats)/std_feats\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if get_moments:\n",
    "            return feats, mean_feats, std_feats\n",
    "        return feats\n",
    "\n",
    "    def get_negative_training_edge(self, poss_nodes, num_nodes, lower_adj):\n",
    "        \"\"\"\n",
    "        Sample negative training edges.\n",
    "        \"\"\"\n",
    "        keep_search = True\n",
    "        while keep_search: # sampled a positive edge\n",
    "            v = np.random.randint(num_nodes)\n",
    "            u = np.random.randint(num_nodes)\n",
    "\n",
    "            keep_search = lower_adj[v, u] == 1 or lower_adj[u, v] == 1\n",
    "\n",
    "        # assert lower_adj[v_sample, s_sample] == 0\n",
    "        # assert u_sample < v_sample; assert u < v;  assert u != v\n",
    "\n",
    "        return u,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model/__init__.py\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "def weight_variable_he_init(input_dim, output_dim, name):\n",
    "    \"\"\"MSRA or He init\"\"\"\n",
    "    return tf.get_variable(name, [input_dim, output_dim],\n",
    "                    initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "def weight_variable_truncated_normal(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with truncated normal distribution, values\n",
    "    that are more than 2 stddev away from the mean are redrawn.\"\"\"\n",
    "\n",
    "    initial = tf.truncated_normal([input_dim, output_dim], stddev=0.5)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_random_uniform(input_dim, output_dim=None, name=\"\"):\n",
    "    \"\"\"Create a weight variable with variables drawn from a\n",
    "    random uniform distribution. Parameters used are taken from paper by\n",
    "    Xavier Glorot and Yoshua Bengio:\n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\"\"\"\n",
    "    if output_dim is not None:\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    else:\n",
    "        init_range = np.sqrt(6.0 / input_dim)\n",
    "        initial = tf.random_uniform([input_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_random_uniform_relu(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with variables drawn from a\n",
    "    random uniform distribution. Parameters used are taken from paper by\n",
    "    Xavier Glorot and Yoshua Bengio:\n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "    and are optimized for ReLU activation function.\"\"\"\n",
    "\n",
    "    init_range = np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_truncated_normal(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.5)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_zero(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as zero.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_one(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def orthogonal(shape, scale=1.1, name=None):\n",
    "    \"\"\"\n",
    "    From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "\n",
    "    # pick the one with the correct shape\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return tf.Variable(scale * q[:shape[0], :shape[1]], name=name, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def bias_variable_const(shape, val, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as zero.\"\"\"\n",
    "    value = tf.to_float(val)\n",
    "    initial = tf.fill(shape, value, name=name)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual-compatibility/model/layers.py\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "            Layers with common name share variables. (TODO)\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, input):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/input', input)\n",
    "            outputs = self._call(input)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, is_train, dropout=0., act=tf.nn.relu,\n",
    "                 bias=False, batch_norm=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_random_uniform(input_dim, output_dim, name=\"weights\")\n",
    "\n",
    "            if bias:\n",
    "                self.vars['node_bias'] = bias_variable_zero([output_dim], name=\"bias_n\")\n",
    "\n",
    "\n",
    "        self.bias = bias\n",
    "        self.batch_norm = batch_norm\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, input):\n",
    "        x_n = input\n",
    "        x_n = tf.nn.dropout(x_n, 1 - self.dropout)\n",
    "        x_n = tf.matmul(x_n, self.vars['weights'])\n",
    "\n",
    "        if self.bias and not self.batch_norm: # do not use bias if using bn\n",
    "            x_n += self.vars['node_bias']\n",
    "\n",
    "        n_outputs = self.act(x_n)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            n_outputs = tf.layers.batch_normalization(n_outputs, training=self.is_train)\n",
    "\n",
    "        return n_outputs\n",
    "\n",
    "    def __call__(self, input):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/input', input)\n",
    "            outputs_n = self._call(input)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_n', outputs_n)\n",
    "            return outputs_n\n",
    "\n",
    "\n",
    "class GCN(Layer):\n",
    "    \"\"\"Graph convolution layer for multiple degree adjacencies\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, support, num_support, is_train, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False, batch_norm=False, init='def', **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        assert init in ['def', 'he']\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if init == 'def':\n",
    "                init_func = weight_variable_random_uniform\n",
    "            else:\n",
    "                init_func = weight_variable_he_init\n",
    "\n",
    "            \n",
    "            self.vars['weights'] = [init_func(input_dim, output_dim,\n",
    "                                            name='weights_n_%d' % i)\n",
    "                                            for i in range(num_support)]\n",
    "\n",
    "            if bias:\n",
    "                self.vars['bias_n'] = bias_variable_zero([output_dim], name=\"bias_n\")\n",
    "\n",
    "            self.weights = self.vars['weights']\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.bias = bias\n",
    "        # TODO, REMOVE\n",
    "        # support = tf.sparse_split(axis=1, num_split=num_support, sp_input=support)\n",
    "        self.support = support\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, input):\n",
    "        x_n = tf.nn.dropout(input, 1 - self.dropout)\n",
    "\n",
    "        supports_n = []\n",
    "\n",
    "        for i in range(len(self.support)):\n",
    "            wn = self.weights[i]\n",
    "            # multiply feature matrices with weights\n",
    "            tmp_n = dot(x_n, wn, sparse=self.sparse_inputs)\n",
    "\n",
    "            support = self.support[i]\n",
    "\n",
    "            # then multiply with rating matrices\n",
    "            supports_n.append(tf.sparse_tensor_dense_matmul(support, tmp_n))\n",
    "\n",
    "        z_n = tf.add_n(supports_n)\n",
    "\n",
    "        if self.bias:\n",
    "            z_n = tf.nn.bias_add(z_n, self.vars['bias_n'])\n",
    "\n",
    "        n_outputs = self.act(z_n)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            n_outputs = tf.layers.batch_normalization(n_outputs, training=self.is_train)\n",
    "\n",
    "        return n_outputs\n",
    "\n",
    "    def __call__(self, input):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/input', input)\n",
    "            outputs_n = self._call(input)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_n', outputs_n)\n",
    "            return outputs_n\n",
    "\n",
    "\n",
    "class MLPDecoder(Layer):\n",
    "    \"\"\"\n",
    "    MLP-based decoder model layer for edge-prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, r_indices, c_indices, input_dim,\n",
    "                 dropout=0., act=lambda x: x, n_out=1, use_bias=False, **kwargs):\n",
    "        super(MLPDecoder, self).__init__(**kwargs)\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_random_uniform(input_dim, n_out, name='weights')\n",
    "            if use_bias:\n",
    "                self.vars['bias'] = bias_variable_zero([n_out], name=\"bias\")\n",
    "\n",
    "        self.r_indices = r_indices\n",
    "        self.c_indices = c_indices\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.n_out = n_out\n",
    "        self.use_bias = use_bias\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        node_inputs = tf.nn.dropout(inputs, 1 - self.dropout)\n",
    "\n",
    "        # r corresponds to the selected rows, and c to the selected columns\n",
    "        row_inputs = tf.gather(node_inputs, self.r_indices)\n",
    "        col_inputs = tf.gather(node_inputs, self.c_indices)\n",
    "\n",
    "        diff = tf.abs(row_inputs - col_inputs)\n",
    "\n",
    "        outputs = tf.matmul(diff, self.vars['weights'])\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs += self.vars['bias']\n",
    "\n",
    "        if self.n_out == 1:\n",
    "            outputs = tf.squeeze(outputs) # remove single dimension\n",
    "\n",
    "        outputs = self.act(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "def softmax_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for multiclass model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth labelt\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def sigmoid_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for binary class model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth label\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    # if pred > 0 then sigmoid(pred) > 0.5\n",
    "    correct_prediction = tf.equal(tf.cast(preds >= 0.0, tf.int64), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for binary class model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth label\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.cast(preds >= 0.5, tf.int64), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def softmax_confusion_matrix(preds, labels):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix. The rows are real labels, and columns the\n",
    "    predictions.\n",
    "    \"\"\"\n",
    "    int_preds = preds >= 0.0\n",
    "    int_preds = tf.cast(int_preds, tf.int32)\n",
    "\n",
    "    return tf.confusion_matrix(labels, int_preds)\n",
    "\n",
    "def softmax_cross_entropy(outputs, labels):\n",
    "    \"\"\" computes average softmax cross entropy \"\"\"\n",
    "\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=labels)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def sigmoid_cross_entropy(outputs, labels):\n",
    "    \"\"\" computes average binary cross entropy \"\"\"\n",
    "\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=outputs, labels=labels)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def binary_cross_entropy(outputs, labels):\n",
    "    # clip values to avoid having log(0)\n",
    "    eps = 1e-4\n",
    "    outputs = tf.clip_by_value(outputs, eps, 1-eps)\n",
    "    cross_entropy = tf.reduce_mean(labels * -tf.log(outputs) + (1-labels) * -tf.log(1-outputs))\n",
    "\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/CompatibilityGAE.py\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'wd'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.total_loss = 0 # to use with weight decay\n",
    "        self.accuracy = 0\n",
    "        self.confmat = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        if 'wd' in kwargs.keys():\n",
    "            self.wd = kwargs.get('wd')\n",
    "        else:\n",
    "            self.wd = 0.\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._confmat()\n",
    "\n",
    "        if self.wd:\n",
    "            reg_weights = tf.get_collection(\"l2_regularize\")\n",
    "            loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in reg_weights ]) * self.wd\n",
    "            self.total_loss += self.loss + loss_l2\n",
    "        else:\n",
    "            self.total_loss = self.loss\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            reg_weights = tf.get_collection(\"l2_regularize\")\n",
    "            self.opt_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CompatibilityGAE(Model):\n",
    "    def __init__(self, placeholders, input_dim, num_classes, num_support,\n",
    "                 learning_rate, hidden, batch_norm=False,\n",
    "                 multi=False, init='def', **kwargs):\n",
    "        super(CompatibilityGAE, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['node_features']\n",
    "        self.support = placeholders['support']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.labels = placeholders['labels']\n",
    "        self.r_indices = placeholders['row_indices']\n",
    "        self.c_indices = placeholders['col_indices']\n",
    "        self.is_train = placeholders['is_train']\n",
    "\n",
    "        self.hidden = hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.num_support = num_support\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_norm = batch_norm\n",
    "        self.init = init\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1.e-8)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"\n",
    "        For mlp decoder.\n",
    "        \"\"\"\n",
    "        self.loss += sigmoid_cross_entropy(self.outputs, self.labels)\n",
    "\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "    def _confmat(self):\n",
    "        self.confmat += softmax_confusion_matrix(self.outputs, self.labels)\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = sigmoid_accuracy(self.outputs, self.labels)\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.cast(self.outputs >= 0.0, tf.int64)\n",
    "\n",
    "    def _build(self):\n",
    "        input_dim = self.input_dim\n",
    "        act_funct = tf.nn.relu\n",
    "        # stack of GCN layers as the encoder\n",
    "        for l in range(len(self.hidden)):\n",
    "            self.layers.append(GCN(input_dim=input_dim,\n",
    "                                     output_dim=self.hidden[l],\n",
    "                                     support=self.support,\n",
    "                                     num_support=self.num_support,\n",
    "                                     act=act_funct,\n",
    "                                     bias=not self.batch_norm,\n",
    "                                     dropout=self.dropout,\n",
    "                                     logging=self.logging,\n",
    "                                     batch_norm=self.batch_norm,\n",
    "                                     is_train=self.is_train,\n",
    "                                     init=self.init))\n",
    "            input_dim = self.hidden[l]\n",
    "\n",
    "        input_dim = self.hidden[-1]\n",
    "\n",
    "        # this is the decoder\n",
    "        self.layers.append(MLPDecoder(num_classes=self.num_classes,\n",
    "                                           r_indices=self.r_indices,\n",
    "                                           c_indices=self.c_indices,\n",
    "                                           input_dim=input_dim,\n",
    "                                           dropout=0.,\n",
    "                                           act=lambda x: x,\n",
    "                                           logging=self.logging,\n",
    "                                           n_out=1,\n",
    "                                           use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  visual-compatibility/utils.py \n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def construct_feed_dict(placeholders, node_features, support, labels, r_indices, c_indices,\n",
    "                        dropout, is_train=True):\n",
    "    \"\"\"\n",
    "    Create feed dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    if not type(support[0]) == tuple:\n",
    "        support = [sparse_to_tuple(sup) for sup in support]\n",
    "\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['node_features']: node_features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['row_indices']: r_indices})\n",
    "    feed_dict.update({placeholders['col_indices']: c_indices})\n",
    "\n",
    "    feed_dict.update({placeholders['dropout']: dropout})\n",
    "    feed_dict.update({placeholders['is_train']: is_train})\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "def support_dropout(sup, do, edge_drop=False):\n",
    "    before = time.time()\n",
    "    sup = sp.tril(sup)\n",
    "    assert do > 0.0 and do < 1.0\n",
    "    n_nodes = sup.shape[0]\n",
    "    # nodes that I want to isolate\n",
    "    isolate = np.random.choice(range(n_nodes), int(n_nodes*do), replace=False)\n",
    "    nnz_rows, nnz_cols = sup.nonzero()\n",
    "\n",
    "    # mask the nodes that have been selected\n",
    "    mask = np.in1d(nnz_rows, isolate)\n",
    "    mask += np.in1d(nnz_cols, isolate)\n",
    "    assert mask.shape[0] == sup.data.shape[0]\n",
    "\n",
    "    sup.data[mask] = 0\n",
    "    sup.eliminate_zeros()\n",
    "\n",
    "    if edge_drop:\n",
    "        prob = np.random.uniform(0, 1, size=sup.data.shape)\n",
    "        remove = prob < do\n",
    "        sup.data[remove] = 0\n",
    "        sup.eliminate_zeros()\n",
    "\n",
    "    sup = sup + sup.transpose()\n",
    "    return sup\n",
    "\n",
    "def write_log(data, logfile):\n",
    "    with open(logfile, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def get_degree_supports(adj, k, adj_self_con=False, verbose=True):\n",
    "    if verbose:\n",
    "        print('Computing adj matrices up to {}th degree'.format(k))\n",
    "    supports = [sp.identity(adj.shape[0])]\n",
    "    if k == 0: # return Identity matrix (no message passing)\n",
    "        return supports\n",
    "    assert k > 0\n",
    "    supports = [sp.identity(adj.shape[0]), adj.astype(np.float64) + adj_self_con*sp.identity(adj.shape[0])]\n",
    "\n",
    "    prev_power = adj\n",
    "    for i in range(k-1):\n",
    "        pow = prev_power.dot(adj)\n",
    "        new_adj = ((pow) == 1).astype(np.float64)\n",
    "        new_adj.setdiag(0)\n",
    "        new_adj.eliminate_zeros()\n",
    "        supports.append(new_adj)\n",
    "        prev_power = pow\n",
    "    return supports\n",
    "\n",
    "def normalize_nonsym_adj(adj):\n",
    "    degree = np.asarray(adj.sum(1)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree[degree == 0.] = np.inf\n",
    "\n",
    "    degree_inv_sqrt = 1. / np.sqrt(degree)\n",
    "    degree_inv_sqrt_mat = sp.diags([degree_inv_sqrt], [0])\n",
    "\n",
    "    degree_inv = degree_inv_sqrt_mat.dot(degree_inv_sqrt_mat)\n",
    "\n",
    "    adj_norm = degree_inv.dot(adj)\n",
    "\n",
    "    return adj_norm\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\" change of format for sparse matrix. This format is used\n",
    "    for the feed_dict where sparse matrices need to be linked to placeholders\n",
    "    representing sparse matrices. \"\"\"\n",
    "\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "class Graph(object):\n",
    "    \"\"\"docstring for Graph.\"\"\"\n",
    "    def __init__(self, adj):\n",
    "        super(Graph, self).__init__()\n",
    "        self.adj = adj\n",
    "        self.n_nodes = adj.shape[0]\n",
    "        self.level = 0\n",
    "\n",
    "    def run_K_BFS(self, n, K):\n",
    "        \"\"\"\n",
    "        Returns a list of K edges, sampled using BFS starting from n\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        edges = []\n",
    "        self.BFS(n, visited, K, edges)\n",
    "        assert len(edges) <= K\n",
    "\n",
    "        return edges\n",
    "\n",
    "    def BFS(self, n, visited, K, edges):\n",
    "        queue = [n]\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                neighs = list(self.adj[node].nonzero()[1])\n",
    "                for neigh in neighs:\n",
    "                    if neigh not in visited:\n",
    "                        edges.append((node, neigh))\n",
    "                        queue.append(neigh)\n",
    "                    if len(edges) == K:\n",
    "                        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import namedtuple\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "def test_amazon(args):\n",
    "    args = namedtuple(\"Args\", args.keys())(*args.values())\n",
    "\n",
    "    load_from = 'D:/School/FARS/Dataset/'+cat_name+'/'+subcat_name+'/logs'\n",
    "    config_file = load_from + '/results.json'\n",
    "    log_file = load_from + '/log.json'\n",
    "\n",
    "    with open(config_file) as f:\n",
    "        config = json.load(f)\n",
    "    with open(log_file) as f:\n",
    "        log = json.load(f)\n",
    "\n",
    "    NUMCLASSES = 2 \n",
    "    BN_AS_TRAIN = False\n",
    "    ADJ_SELF_CONNECTIONS = True\n",
    "\n",
    "    # evaluate in the specified version\n",
    "    print(\"Trained with {}, evaluating with {}\")\n",
    "    cat_rel = args.amz_data\n",
    "    dp = DataLoaderAmazon(cat_rel=cat_rel)\n",
    "    train_features, adj_train, train_labels, train_r_indices, train_c_indices = dp.get_phase('train')\n",
    "    _, adj_val, val_labels, val_r_indices, val_c_indices = dp.get_phase('valid')\n",
    "    _, adj_test, test_labels, test_r_indices, test_c_indices = dp.get_phase('test')\n",
    "    full_adj = dp.adj\n",
    "\n",
    "    def norm_adj(adj_to_norm):\n",
    "        return normalize_nonsym_adj(adj_to_norm)\n",
    "\n",
    "    train_features, mean, std = dp.normalize_features(train_features, get_moments=True)\n",
    "\n",
    "    train_support = get_degree_supports(adj_train, config['degree'], adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "    val_support = get_degree_supports(adj_val, config['degree'], adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "    test_support = get_degree_supports(adj_test, config['degree'], adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "\n",
    "    for i in range(1, len(train_support)):\n",
    "        train_support[i] = norm_adj(train_support[i])\n",
    "        val_support[i] = norm_adj(val_support[i])\n",
    "        test_support[i] = norm_adj(test_support[i])\n",
    "\n",
    "    num_support = len(train_support)\n",
    "\n",
    "    num_support = len(train_support)\n",
    "    placeholders = {\n",
    "        'row_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "        'col_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "        'weight_decay': tf.placeholder_with_default(0., shape=()),\n",
    "        'is_train': tf.placeholder_with_default(True, shape=()),\n",
    "        'support': [tf.sparse_placeholder(tf.float32, shape=(None, None)) for sup in range(num_support)],\n",
    "        'node_features': tf.placeholder(tf.float32, shape=(None, None)),\n",
    "        'labels': tf.placeholder(tf.float32, shape=(None,))   \n",
    "    }\n",
    "\n",
    "    model = CompatibilityGAE(placeholders,\n",
    "                    input_dim=train_features.shape[1],\n",
    "                    num_classes=NUMCLASSES,\n",
    "                    num_support=num_support,\n",
    "                    hidden=config['hidden'],\n",
    "                    learning_rate=config['learning_rate'],\n",
    "                    logging=True,\n",
    "                    batch_norm=config['batch_norm'])\n",
    "\n",
    "    train_feed_dict = construct_feed_dict(placeholders, train_features, train_support,\n",
    "                                          train_labels, train_r_indices, train_c_indices, config['dropout'])\n",
    "    # No dropout for validation and test runs\n",
    "    val_feed_dict = construct_feed_dict(placeholders, train_features, val_support,\n",
    "                                        val_labels, val_r_indices, val_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "    test_feed_dict = construct_feed_dict(placeholders, train_features, test_support,\n",
    "                                         test_labels, test_r_indices, test_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, load_from+'/'+'best_epoch.ckpt')\n",
    "\n",
    "        val_avg_loss, val_acc, conf, pred = sess.run([model.loss, model.accuracy, model.confmat, model.predict()], feed_dict=val_feed_dict)\n",
    "\n",
    "        print(\"val_loss=\", \"{:.5f}\".format(val_avg_loss),\n",
    "              \"val_acc=\", \"{:.5f}\".format(val_acc))\n",
    "\n",
    "        test_avg_loss, test_acc, conf = sess.run([model.loss, model.accuracy, model.confmat], feed_dict=test_feed_dict)\n",
    "\n",
    "        print(\"test_loss=\", \"{:.5f}\".format(test_avg_loss),\n",
    "              \"test_acc=\", \"{:.5f}\".format(test_acc))\n",
    "\n",
    "        # rerun for K=0 (all in parallel)\n",
    "        k_0_adj = sp.csr_matrix(adj_val.shape)\n",
    "        k_0_support = get_degree_supports(k_0_adj, 1, adj_self_con=ADJ_SELF_CONNECTIONS, verbose=False)\n",
    "        for i in range(1, len(k_0_support)):\n",
    "            k_0_support[i] = norm_adj(k_0_support[i])\n",
    "        k_0_support = [sparse_to_tuple(sup) for sup in k_0_support]\n",
    "\n",
    "        k_0_val_feed_dict = construct_feed_dict(placeholders, train_features, k_0_support,\n",
    "                                            val_labels, val_r_indices, val_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "        k_0_test_feed_dict = construct_feed_dict(placeholders, train_features, k_0_support,\n",
    "                                            test_labels, test_r_indices, test_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "\n",
    "        val_avg_loss, val_acc, conf, pred = sess.run([model.loss, model.accuracy, model.confmat, model.predict()], feed_dict=k_0_val_feed_dict)\n",
    "        print(conf.shape)\n",
    "        print(conf)\n",
    "        print(pred.shape)\n",
    "        print(pred)\n",
    "        print(\"for k=0 val_loss=\", \"{:.5f}\".format(val_avg_loss),\n",
    "              \"for k=0 val_acc=\", \"{:.5f}\".format(val_acc))\n",
    "\n",
    "        test_avg_loss, test_acc, conf = sess.run([model.loss, model.accuracy, model.confmat], feed_dict=k_0_test_feed_dict)\n",
    "        print(\"for k=0 test_loss=\", \"{:.5f}\".format(test_avg_loss),\n",
    "              \"for k=0 test_acc=\", \"{:.5f}\".format(test_acc))\n",
    "\n",
    "        lst = args.k\n",
    "        for K in lst:\n",
    "            available_adj = dp.full_valid_adj + dp.full_train_adj\n",
    "            available_adj = available_adj.tolil()\n",
    "            for r,c in zip(test_r_indices.astype(int), test_c_indices.astype(int)):\n",
    "                available_adj[r,c] = 0\n",
    "                available_adj[c,r] = 0\n",
    "            available_adj = available_adj.tocsr()\n",
    "            available_adj.eliminate_zeros()\n",
    "\n",
    "            G = Graph(available_adj)\n",
    "            get_edges_func = G.run_K_BFS\n",
    "\n",
    "            new_adj = sp.csr_matrix(full_adj.shape)\n",
    "            new_adj = new_adj.tolil()\n",
    "            for r,c in zip(test_r_indices.astype(int), test_c_indices.astype(int)):\n",
    "                before = time.time()\n",
    "                if K > 0: #expand the edges\n",
    "                    nodes_to_expand = [r,c]\n",
    "                    for node in nodes_to_expand:\n",
    "                        edges = get_edges_func(node, K)\n",
    "                        for edge in edges:\n",
    "                            i, j = edge\n",
    "                            new_adj[i, j] = 1\n",
    "                            new_adj[j, i] = 1\n",
    "\n",
    "            new_adj = new_adj.tocsr()\n",
    "\n",
    "            new_support = get_degree_supports(new_adj, config['degree'], adj_self_con=ADJ_SELF_CONNECTIONS, verbose=False)\n",
    "            for i in range(1, len(new_support)):\n",
    "                new_support[i] = norm_adj(new_support[i])\n",
    "            new_support = [sparse_to_tuple(sup) for sup in new_support]\n",
    "\n",
    "            new_feed_dict = construct_feed_dict(placeholders, train_features, new_support,\n",
    "                                test_labels, test_r_indices, test_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "\n",
    "            loss, acc = sess.run([model.loss, model.accuracy], feed_dict=new_feed_dict)\n",
    "\n",
    "            print(\"for k={} test_acc=\".format(K), \"{:.5f}\".format(acc))\n",
    "\n",
    "    print('Best val score saved in log: {}'.format(config['best_val_score']))\n",
    "    print('Last val score saved in log: {}'.format(log['val']['acc'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained with {}, evaluating with {}\n",
      "initializing dataloader...\n",
      "get phase: train\n",
      "Sampling negative edges...\n",
      "Sampling done, time elapsed: 5.915739059448242\n",
      "get phase: valid\n",
      "Sampling negative edges...\n",
      "Sampling done, time elapsed: 1.546630859375\n",
      "get phase: test\n",
      "Sampling negative edges...\n",
      "Sampling done, time elapsed: 1.63966965675354\n",
      "Computing adj matrices up to 1th degree\n",
      "Computing adj matrices up to 1th degree\n",
      "Computing adj matrices up to 1th degree\n",
      "WARNING:tensorflow:From C:\\Users\\Malhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Malhi\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malhi\\AppData\\Local\\Temp\\ipykernel_1140\\2890050926.py:177: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  n_outputs = tf.layers.batch_normalization(n_outputs, training=self.is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Malhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Restoring parameters from D:/School/FARS/Dataset/Men/Men_also_bought/logs/best_epoch.ckpt\n",
      "val_loss= 0.26194 val_acc= 0.89343\n",
      "test_loss= 0.25205 test_acc= 0.89950\n",
      "(2, 2)\n",
      "[[34787    69]\n",
      " [33762  1094]]\n",
      "(69712,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "for k=0 val_loss= 1.45299 for k=0 val_acc= 0.51470\n",
      "for k=0 test_loss= 1.45480 for k=0 test_acc= 0.51409\n",
      "for k=1 test_acc= 0.73665\n",
      "for k=5 test_acc= 0.93523\n",
      "for k=10 test_acc= 0.95976\n",
      "for k=20 test_acc= 0.96726\n",
      "for k=30 test_acc= 0.96854\n",
      "for k=40 test_acc= 0.96868\n",
      "for k=50 test_acc= 0.96872\n",
      "for k=60 test_acc= 0.96873\n",
      "for k=70 test_acc= 0.96865\n",
      "for k=80 test_acc= 0.96865\n",
      "for k=90 test_acc= 0.96868\n",
      "for k=100 test_acc= 0.96872\n",
      "Best val score saved in log: 0.8429498076438904\n",
      "Last val score saved in log: 0.836219072341919\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import namedtuple\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\"\")\n",
    "    parser.add_argument(\"-k\", type=int, default=[1,5,10,20,30,40,50,60,70,80,90,100],\n",
    "                    help=\"K used for the variable number of edges case\")\n",
    "    parser.add_argument(\"-lf\", \"--load_from\", type=str, help=\"Model used.\")\n",
    "    parser.add_argument(\"-amzd\", \"--amz_data\", type=str, default=subcat_name,\n",
    "            choices=[subcat_name],\n",
    "            help=\"Dataset string.\")\n",
    "    args = parser.parse_args()\n",
    "    test_amazon(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
