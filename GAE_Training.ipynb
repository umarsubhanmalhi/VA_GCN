{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_name=\"Men\"\n",
    "subcat_name=\"Men_also_bought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TODO: clean unused code\n",
    "\n",
    "class DataLoaderAmazon(object):\n",
    "    \"\"\"\n",
    "    Load amazon data.\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_rel='bought_together'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            normalize: normalize the features or not\n",
    "            cat_rel: category and type of relation used\n",
    "        \"\"\"\n",
    "        super(DataLoaderAmazon, self).__init__()\n",
    "        self.cat_rel = cat_rel\n",
    "\n",
    "        self.path_dataset = 'D:/School/FARS/Dataset/'+cat_name+'/'+cat_rel+'/'\n",
    "        assert os.path.exists(self.path_dataset)\n",
    "\n",
    "        print('initializing dataloader...')\n",
    "        self.init_dataset()\n",
    "\n",
    "    def init_dataset(self):\n",
    "        path_dataset = self.path_dataset\n",
    "        adj_file = path_dataset + 'adj.npz'\n",
    "        feats_file = path_dataset + 'deep_feats.npy'\n",
    "        np.random.seed(1234)\n",
    "\n",
    "        self.adj = sp.load_npz(adj_file).astype(np.int32)\n",
    "        node_features = np.load(feats_file)\n",
    "        self.features = node_features\n",
    "\n",
    "        # get lower tiangle of the adj matrix to avoid duplicate edges\n",
    "        self.lower_adj = sp.tril(self.adj).tocsr()\n",
    "\n",
    "        # get positive edges and split them into train, val and test\n",
    "        pos_r_idx, pos_c_idx = self.lower_adj.nonzero()\n",
    "        pos_labels = np.array(self.lower_adj[pos_r_idx, pos_c_idx]).squeeze()\n",
    "\n",
    "        n_pos = pos_labels.shape[0] # number of positive edges\n",
    "        perm = list(range(n_pos))\n",
    "        np.random.shuffle(perm)\n",
    "        pos_labels, pos_r_idx, pos_c_idx = pos_labels[perm], pos_r_idx[perm], pos_c_idx[perm]\n",
    "        n_train = int(n_pos*0.80)\n",
    "        n_val = int(n_pos*0.10)\n",
    "\n",
    "        self.train_pos_labels, self.train_pos_r_idx, self.train_pos_c_idx = pos_labels[:n_train], pos_r_idx[:n_train], pos_c_idx[:n_train]\n",
    "        self.val_pos_labels, self.val_pos_r_idx, self.val_pos_c_idx = pos_labels[n_train:n_train + n_val], pos_r_idx[n_train:n_train + n_val], pos_c_idx[n_train:n_train + n_val]\n",
    "        self.test_pos_labels, self.test_pos_r_idx, self.test_pos_c_idx = pos_labels[n_train + n_val:], pos_r_idx[n_train + n_val:], pos_c_idx[n_train + n_val:]\n",
    "\n",
    "    def get_phase(self, phase):\n",
    "        print('get phase: {}'.format(phase))\n",
    "        assert phase in ['train', 'valid', 'test']\n",
    "\n",
    "        lower_adj = self.lower_adj\n",
    "\n",
    "        # get the positive edges\n",
    "\n",
    "        if phase == 'train':\n",
    "            pos_labels, pos_r_idx, pos_c_idx = self.train_pos_labels, self.train_pos_r_idx, self.train_pos_c_idx\n",
    "        elif phase == 'valid':\n",
    "            pos_labels, pos_r_idx, pos_c_idx = self.val_pos_labels, self.val_pos_r_idx, self.val_pos_c_idx\n",
    "        elif phase == 'test':\n",
    "            pos_labels, pos_r_idx, pos_c_idx = self.test_pos_labels, self.test_pos_r_idx, self.test_pos_c_idx\n",
    "\n",
    "        # build adj matrix\n",
    "        full_adj = sp.csr_matrix((\n",
    "                    np.hstack([pos_labels, pos_labels]),\n",
    "                    (np.hstack([pos_r_idx, pos_c_idx]), np.hstack([pos_c_idx, pos_r_idx]))\n",
    "                ),\n",
    "                shape=(lower_adj.shape[0], lower_adj.shape[0])\n",
    "            )\n",
    "        setattr(self, 'full_{}_adj'.format(phase), full_adj)\n",
    "\n",
    "        # split the positive edges into the ones used for evaluation and the ones used as message passing\n",
    "        n_pos = pos_labels.shape[0] # number of positive edges\n",
    "        n_eval = int(n_pos/2)\n",
    "        mp_pos_labels, mp_pos_r_idx, mp_pos_c_idx = pos_labels[n_eval:], pos_r_idx[n_eval:], pos_c_idx[n_eval:]\n",
    "        # this are the positive examples that will be used to compute the loss function\n",
    "        eval_pos_labels, eval_pos_r_idx, eval_pos_c_idx = pos_labels[:n_eval], pos_r_idx[:n_eval], pos_c_idx[:n_eval]\n",
    "\n",
    "        # get the negative edges\n",
    "\n",
    "        print('Sampling negative edges...')\n",
    "        before = time.time()\n",
    "        n_train_neg = eval_pos_labels.shape[0] # set the number of negative training edges that will be needed to sample at each iter\n",
    "        neg_labels = np.zeros((n_train_neg))\n",
    "        # get the possible indexes to be sampled (basically all indexes if there aren't restrictions)\n",
    "        poss_nodes = np.arange(lower_adj.shape[0])\n",
    "\n",
    "        neg_r_idx = np.zeros((n_train_neg))\n",
    "        neg_c_idx = np.zeros((n_train_neg))\n",
    "\n",
    "        for i in range(n_train_neg):\n",
    "            r_idx, c_idx = self.get_negative_training_edge(poss_nodes, poss_nodes.shape[0], lower_adj)\n",
    "            neg_r_idx[i] = r_idx\n",
    "            neg_c_idx[i] = c_idx\n",
    "        print('Sampling done, time elapsed: {}'.format(time.time() - before))\n",
    "\n",
    "        # build adj matrix\n",
    "        adj = sp.csr_matrix((\n",
    "                    np.hstack([mp_pos_labels, mp_pos_labels]),\n",
    "                    (np.hstack([mp_pos_r_idx, mp_pos_c_idx]), np.hstack([mp_pos_c_idx, mp_pos_r_idx]))\n",
    "                ),\n",
    "                shape=(lower_adj.shape[0], lower_adj.shape[0])\n",
    "            )\n",
    "        # remove the labels of the negative edges which are 0\n",
    "        adj.eliminate_zeros()\n",
    "\n",
    "        labels = np.append(eval_pos_labels, neg_labels)\n",
    "        r_idx = np.append(eval_pos_r_idx, neg_r_idx)\n",
    "        c_idx = np.append(eval_pos_c_idx, neg_c_idx)\n",
    "\n",
    "        return self.features, adj, labels, r_idx, c_idx\n",
    "\n",
    "    def normalize_features(self, feats, get_moments=False, mean=None, std=None):\n",
    "        reuse_mean = mean is not None and std is not None\n",
    "        if feats.shape[1] == 256: # image features\n",
    "            if reuse_mean:\n",
    "                mean_feats = mean\n",
    "                std_feats = std\n",
    "            else:\n",
    "                mean_feats = feats.mean(axis=0)\n",
    "                std_feats = feats.std(axis=0)\n",
    "\n",
    "            # normalize\n",
    "            feats = (feats - mean_feats)/std_feats\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if get_moments:\n",
    "            return feats, mean_feats, std_feats\n",
    "        return feats\n",
    "\n",
    "    def get_negative_training_edge(self, poss_nodes, num_nodes, lower_adj):\n",
    "        \"\"\"\n",
    "        Sample negative training edges.\n",
    "        \"\"\"\n",
    "        keep_search = True\n",
    "        while keep_search: # sampled a positive edge\n",
    "            v = np.random.randint(num_nodes)\n",
    "            u = np.random.randint(num_nodes)\n",
    "\n",
    "            keep_search = lower_adj[v, u] == 1 or lower_adj[u, v] == 1\n",
    "\n",
    "        # assert lower_adj[v_sample, s_sample] == 0\n",
    "        # assert u_sample < v_sample; assert u < v;  assert u != v\n",
    "\n",
    "        return u,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model/__init__.py\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "def weight_variable_he_init(input_dim, output_dim, name):\n",
    "    \"\"\"MSRA or He init\"\"\"\n",
    "    return tf.get_variable(name, [input_dim, output_dim],\n",
    "                    initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "def weight_variable_truncated_normal(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with truncated normal distribution, values\n",
    "    that are more than 2 stddev away from the mean are redrawn.\"\"\"\n",
    "\n",
    "    initial = tf.truncated_normal([input_dim, output_dim], stddev=0.5)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_random_uniform(input_dim, output_dim=None, name=\"\"):\n",
    "    \"\"\"Create a weight variable with variables drawn from a\n",
    "    random uniform distribution. Parameters used are taken from paper by\n",
    "    Xavier Glorot and Yoshua Bengio:\n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\"\"\"\n",
    "    if output_dim is not None:\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    else:\n",
    "        init_range = np.sqrt(6.0 / input_dim)\n",
    "        initial = tf.random_uniform([input_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_random_uniform_relu(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with variables drawn from a\n",
    "    random uniform distribution. Parameters used are taken from paper by\n",
    "    Xavier Glorot and Yoshua Bengio:\n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "    and are optimized for ReLU activation function.\"\"\"\n",
    "\n",
    "    init_range = np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_truncated_normal(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.5)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_zero(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as zero.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_one(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def orthogonal(shape, scale=1.1, name=None):\n",
    "    \"\"\"\n",
    "    From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "\n",
    "    # pick the one with the correct shape\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return tf.Variable(scale * q[:shape[0], :shape[1]], name=name, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def bias_variable_const(shape, val, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as zero.\"\"\"\n",
    "    value = tf.to_float(val)\n",
    "    initial = tf.fill(shape, value, name=name)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual-compatibility/model/layers.py\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "            Layers with common name share variables. (TODO)\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, input):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/input', input)\n",
    "            outputs = self._call(input)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, is_train, dropout=0., act=tf.nn.relu,\n",
    "                 bias=False, batch_norm=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_random_uniform(input_dim, output_dim, name=\"weights\")\n",
    "\n",
    "            if bias:\n",
    "                self.vars['node_bias'] = bias_variable_zero([output_dim], name=\"bias_n\")\n",
    "\n",
    "\n",
    "        self.bias = bias\n",
    "        self.batch_norm = batch_norm\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, input):\n",
    "        x_n = input\n",
    "        x_n = tf.nn.dropout(x_n, 1 - self.dropout)\n",
    "        x_n = tf.matmul(x_n, self.vars['weights'])\n",
    "\n",
    "        if self.bias and not self.batch_norm: # do not use bias if using bn\n",
    "            x_n += self.vars['node_bias']\n",
    "\n",
    "        n_outputs = self.act(x_n)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            n_outputs = tf.layers.batch_normalization(n_outputs, training=self.is_train)\n",
    "\n",
    "        return n_outputs\n",
    "\n",
    "    def __call__(self, input):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/input', input)\n",
    "            outputs_n = self._call(input)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_n', outputs_n)\n",
    "            return outputs_n\n",
    "\n",
    "\n",
    "class GCN(Layer):\n",
    "    \"\"\"Graph convolution layer for multiple degree adjacencies\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, support, num_support, is_train, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False, batch_norm=False, init='def', **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        assert init in ['def', 'he']\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if init == 'def':\n",
    "                init_func = weight_variable_random_uniform\n",
    "            else:\n",
    "                init_func = weight_variable_he_init\n",
    "\n",
    "            \n",
    "            self.vars['weights'] = [init_func(input_dim, output_dim,\n",
    "                                            name='weights_n_%d' % i)\n",
    "                                            for i in range(num_support)]\n",
    "\n",
    "            if bias:\n",
    "                self.vars['bias_n'] = bias_variable_zero([output_dim], name=\"bias_n\")\n",
    "\n",
    "            self.weights = self.vars['weights']\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.bias = bias\n",
    "        # TODO, REMOVE\n",
    "        # support = tf.sparse_split(axis=1, num_split=num_support, sp_input=support)\n",
    "        self.support = support\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, input):\n",
    "        x_n = tf.nn.dropout(input, 1 - self.dropout)\n",
    "\n",
    "        supports_n = []\n",
    "\n",
    "        for i in range(len(self.support)):\n",
    "            wn = self.weights[i]\n",
    "            # multiply feature matrices with weights\n",
    "            tmp_n = dot(x_n, wn, sparse=self.sparse_inputs)\n",
    "\n",
    "            support = self.support[i]\n",
    "\n",
    "            # then multiply with rating matrices\n",
    "            supports_n.append(tf.sparse_tensor_dense_matmul(support, tmp_n))\n",
    "\n",
    "        z_n = tf.add_n(supports_n)\n",
    "\n",
    "        if self.bias:\n",
    "            z_n = tf.nn.bias_add(z_n, self.vars['bias_n'])\n",
    "\n",
    "        n_outputs = self.act(z_n)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            n_outputs = tf.layers.batch_normalization(n_outputs, training=self.is_train)\n",
    "\n",
    "        return n_outputs\n",
    "\n",
    "    def __call__(self, input):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/input', input)\n",
    "            outputs_n = self._call(input)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_n', outputs_n)\n",
    "            return outputs_n\n",
    "\n",
    "\n",
    "class MLPDecoder(Layer):\n",
    "    \"\"\"\n",
    "    MLP-based decoder model layer for edge-prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, r_indices, c_indices, input_dim,\n",
    "                 dropout=0., act=lambda x: x, n_out=1, use_bias=False, **kwargs):\n",
    "        super(MLPDecoder, self).__init__(**kwargs)\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_random_uniform(input_dim, n_out, name='weights')\n",
    "            if use_bias:\n",
    "                self.vars['bias'] = bias_variable_zero([n_out], name=\"bias\")\n",
    "\n",
    "        self.r_indices = r_indices\n",
    "        self.c_indices = c_indices\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.n_out = n_out\n",
    "        self.use_bias = use_bias\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        node_inputs = tf.nn.dropout(inputs, 1 - self.dropout)\n",
    "\n",
    "        # r corresponds to the selected rows, and c to the selected columns\n",
    "        row_inputs = tf.gather(node_inputs, self.r_indices)\n",
    "        col_inputs = tf.gather(node_inputs, self.c_indices)\n",
    "\n",
    "        diff = tf.abs(row_inputs - col_inputs)\n",
    "\n",
    "        outputs = tf.matmul(diff, self.vars['weights'])\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs += self.vars['bias']\n",
    "\n",
    "        if self.n_out == 1:\n",
    "            outputs = tf.squeeze(outputs) # remove single dimension\n",
    "\n",
    "        outputs = self.act(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "def softmax_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for multiclass model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth labelt\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def sigmoid_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for binary class model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth label\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    # if pred > 0 then sigmoid(pred) > 0.5\n",
    "    correct_prediction = tf.equal(tf.cast(preds >= 0.0, tf.int64), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for binary class model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth label\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.cast(preds >= 0.5, tf.int64), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def softmax_confusion_matrix(preds, labels):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix. The rows are real labels, and columns the\n",
    "    predictions.\n",
    "    \"\"\"\n",
    "    int_preds = preds >= 0.0\n",
    "    int_preds = tf.cast(int_preds, tf.int32)\n",
    "\n",
    "    return tf.confusion_matrix(labels, int_preds)\n",
    "\n",
    "def softmax_cross_entropy(outputs, labels):\n",
    "    \"\"\" computes average softmax cross entropy \"\"\"\n",
    "\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=labels)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def sigmoid_cross_entropy(outputs, labels):\n",
    "    \"\"\" computes average binary cross entropy \"\"\"\n",
    "\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=outputs, labels=labels)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def binary_cross_entropy(outputs, labels):\n",
    "    # clip values to avoid having log(0)\n",
    "    eps = 1e-4\n",
    "    outputs = tf.clip_by_value(outputs, eps, 1-eps)\n",
    "    cross_entropy = tf.reduce_mean(labels * -tf.log(outputs) + (1-labels) * -tf.log(1-outputs))\n",
    "\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/CompatibilityGAE.py\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'wd'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.total_loss = 0 # to use with weight decay\n",
    "        self.accuracy = 0\n",
    "        self.confmat = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        if 'wd' in kwargs.keys():\n",
    "            self.wd = kwargs.get('wd')\n",
    "        else:\n",
    "            self.wd = 0.\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._confmat()\n",
    "\n",
    "        if self.wd:\n",
    "            reg_weights = tf.get_collection(\"l2_regularize\")\n",
    "            loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in reg_weights ]) * self.wd\n",
    "            self.total_loss += self.loss + loss_l2\n",
    "        else:\n",
    "            self.total_loss = self.loss\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            reg_weights = tf.get_collection(\"l2_regularize\")\n",
    "            self.opt_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CompatibilityGAE(Model):\n",
    "    def __init__(self, placeholders, input_dim, num_classes, num_support,\n",
    "                 learning_rate, hidden, batch_norm=False,\n",
    "                 multi=False, init='def', **kwargs):\n",
    "        super(CompatibilityGAE, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['node_features']\n",
    "        self.support = placeholders['support']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.labels = placeholders['labels']\n",
    "        self.r_indices = placeholders['row_indices']\n",
    "        self.c_indices = placeholders['col_indices']\n",
    "        self.is_train = placeholders['is_train']\n",
    "\n",
    "        self.hidden = hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.num_support = num_support\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_norm = batch_norm\n",
    "        self.init = init\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1.e-8)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"\n",
    "        For mlp decoder.\n",
    "        \"\"\"\n",
    "        self.loss += sigmoid_cross_entropy(self.outputs, self.labels)\n",
    "\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "    def _confmat(self):\n",
    "        self.confmat += softmax_confusion_matrix(self.outputs, self.labels)\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = sigmoid_accuracy(self.outputs, self.labels)\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.cast(self.outputs >= 0.0, tf.int64)\n",
    "\n",
    "    def _build(self):\n",
    "        input_dim = self.input_dim\n",
    "        act_funct = tf.nn.relu\n",
    "        # stack of GCN layers as the encoder\n",
    "        for l in range(len(self.hidden)):\n",
    "            self.layers.append(GCN(input_dim=input_dim,\n",
    "                                     output_dim=self.hidden[l],\n",
    "                                     support=self.support,\n",
    "                                     num_support=self.num_support,\n",
    "                                     act=act_funct,\n",
    "                                     bias=not self.batch_norm,\n",
    "                                     dropout=self.dropout,\n",
    "                                     logging=self.logging,\n",
    "                                     batch_norm=self.batch_norm,\n",
    "                                     is_train=self.is_train,\n",
    "                                     init=self.init))\n",
    "            input_dim = self.hidden[l]\n",
    "\n",
    "        input_dim = self.hidden[-1]\n",
    "\n",
    "        # this is the decoder\n",
    "        self.layers.append(MLPDecoder(num_classes=self.num_classes,\n",
    "                                           r_indices=self.r_indices,\n",
    "                                           c_indices=self.c_indices,\n",
    "                                           input_dim=input_dim,\n",
    "                                           dropout=0.,\n",
    "                                           act=lambda x: x,\n",
    "                                           logging=self.logging,\n",
    "                                           n_out=1,\n",
    "                                           use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  visual-compatibility/utils.py \n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def construct_feed_dict(placeholders, node_features, support, labels, r_indices, c_indices,\n",
    "                        dropout, is_train=True):\n",
    "    \"\"\"\n",
    "    Create feed dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    if not type(support[0]) == tuple:\n",
    "        support = [sparse_to_tuple(sup) for sup in support]\n",
    "\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['node_features']: node_features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['row_indices']: r_indices})\n",
    "    feed_dict.update({placeholders['col_indices']: c_indices})\n",
    "\n",
    "    feed_dict.update({placeholders['dropout']: dropout})\n",
    "    feed_dict.update({placeholders['is_train']: is_train})\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "def support_dropout(sup, do, edge_drop=False):\n",
    "    before = time.time()\n",
    "    sup = sp.tril(sup)\n",
    "    assert do > 0.0 and do < 1.0\n",
    "    n_nodes = sup.shape[0]\n",
    "    # nodes that I want to isolate\n",
    "    isolate = np.random.choice(range(n_nodes), int(n_nodes*do), replace=False)\n",
    "    nnz_rows, nnz_cols = sup.nonzero()\n",
    "\n",
    "    # mask the nodes that have been selected\n",
    "    mask = np.in1d(nnz_rows, isolate)\n",
    "    mask += np.in1d(nnz_cols, isolate)\n",
    "    assert mask.shape[0] == sup.data.shape[0]\n",
    "\n",
    "    sup.data[mask] = 0\n",
    "    sup.eliminate_zeros()\n",
    "\n",
    "    if edge_drop:\n",
    "        prob = np.random.uniform(0, 1, size=sup.data.shape)\n",
    "        remove = prob < do\n",
    "        sup.data[remove] = 0\n",
    "        sup.eliminate_zeros()\n",
    "\n",
    "    sup = sup + sup.transpose()\n",
    "    return sup\n",
    "\n",
    "def write_log(data, logfile):\n",
    "    with open(logfile, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def get_degree_supports(adj, k, adj_self_con=False, verbose=True):\n",
    "    if verbose:\n",
    "        print('Computing adj matrices up to {}th degree'.format(k))\n",
    "    supports = [sp.identity(adj.shape[0])]\n",
    "    if k == 0: # return Identity matrix (no message passing)\n",
    "        return supports\n",
    "    assert k > 0\n",
    "    supports = [sp.identity(adj.shape[0]), adj.astype(np.float64) + adj_self_con*sp.identity(adj.shape[0])]\n",
    "\n",
    "    prev_power = adj\n",
    "    for i in range(k-1):\n",
    "        pow = prev_power.dot(adj)\n",
    "        new_adj = ((pow) == 1).astype(np.float64)\n",
    "        new_adj.setdiag(0)\n",
    "        new_adj.eliminate_zeros()\n",
    "        supports.append(new_adj)\n",
    "        prev_power = pow\n",
    "    return supports\n",
    "\n",
    "def normalize_nonsym_adj(adj):\n",
    "    degree = np.asarray(adj.sum(1)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree[degree == 0.] = np.inf\n",
    "\n",
    "    degree_inv_sqrt = 1. / np.sqrt(degree)\n",
    "    degree_inv_sqrt_mat = sp.diags([degree_inv_sqrt], [0])\n",
    "\n",
    "    degree_inv = degree_inv_sqrt_mat.dot(degree_inv_sqrt_mat)\n",
    "\n",
    "    adj_norm = degree_inv.dot(adj)\n",
    "\n",
    "    return adj_norm\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\" change of format for sparse matrix. This format is used\n",
    "    for the feed_dict where sparse matrices need to be linked to placeholders\n",
    "    representing sparse matrices. \"\"\"\n",
    "\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "class Graph(object):\n",
    "    \"\"\"docstring for Graph.\"\"\"\n",
    "    def __init__(self, adj):\n",
    "        super(Graph, self).__init__()\n",
    "        self.adj = adj\n",
    "        self.n_nodes = adj.shape[0]\n",
    "        self.level = 0\n",
    "\n",
    "    def run_K_BFS(self, n, K):\n",
    "        \"\"\"\n",
    "        Returns a list of K edges, sampled using BFS starting from n\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        edges = []\n",
    "        self.BFS(n, visited, K, edges)\n",
    "        assert len(edges) <= K\n",
    "\n",
    "        return edges\n",
    "\n",
    "    def BFS(self, n, visited, K, edges):\n",
    "        queue = [n]\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                neighs = list(self.adj[node].nonzero()[1])\n",
    "                for neigh in neighs:\n",
    "                    if neigh not in visited:\n",
    "                        edges.append((node, neigh))\n",
    "                        queue.append(neigh)\n",
    "                    if len(edges) == K:\n",
    "                        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "{'dataset': 'amazon', 'learning_rate': 0.01, 'weight_decay': 0.0, 'epochs': 300, 'hidden': [256, 128, 64], 'dropout': 0.5, 'degree': 1, 'summaries_dir': 'D:/School/FARS/Dataset/Men/Men_also_bought/logs', 'support_dropout': 0.15, 'write_summary': False, 'batch_norm': True, 'amz_data': 'Men_also_bought'} \n",
      "\n",
      "initializing dataloader...\n",
      "get phase: train\n",
      "Sampling negative edges...\n",
      "Sampling done, time elapsed: 8.231645584106445\n",
      "get phase: valid\n",
      "Sampling negative edges...\n",
      "Sampling done, time elapsed: 0.9850690364837646\n",
      "get phase: test\n",
      "Sampling negative edges...\n",
      "Sampling done, time elapsed: 1.1579179763793945\n",
      "Computing adj matrices up to 1th degree\n",
      "Computing adj matrices up to 1th degree\n",
      "Computing adj matrices up to 1th degree\n",
      "WARNING:tensorflow:From C:\\Users\\Malhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Malhi\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malhi\\AppData\\Local\\Temp\\ipykernel_18372\\2890050926.py:177: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  n_outputs = tf.layers.batch_normalization(n_outputs, training=self.is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Malhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Training...\n",
      "[*] Epoch: 0001 train_loss= 0.89660 train_acc= 0.49075 val_loss= 3.33368 val_acc= 0.50012 \t\ttime= 2.56440\n",
      "[*] Epoch: 0002 train_loss= 0.86626 train_acc= 0.61596 val_loss= 3.60583 val_acc= 0.50015 \t\ttime= 1.74770\n",
      "[*] Epoch: 0003 train_loss= 0.89652 train_acc= 0.56759 val_loss= 3.25967 val_acc= 0.50041 \t\ttime= 1.74864\n",
      "[*] Epoch: 0004 train_loss= 0.64313 train_acc= 0.66064 val_loss= 3.61709 val_acc= 0.50315 \t\ttime= 2.23709\n",
      "[*] Epoch: 0005 train_loss= 0.63108 train_acc= 0.67884 val_loss= 4.21360 val_acc= 0.50217 \t\ttime= 1.81747\n",
      "[*] Epoch: 0006 train_loss= 0.59272 train_acc= 0.70171 val_loss= 4.94451 val_acc= 0.50239 \t\ttime= 2.05341\n",
      "[*] Epoch: 0007 train_loss= 0.56435 train_acc= 0.71959 val_loss= 5.58396 val_acc= 0.50256 \t\ttime= 1.77487\n",
      "[*] Epoch: 0008 train_loss= 0.54131 train_acc= 0.73302 val_loss= 6.00556 val_acc= 0.50198 \t\ttime= 1.80995\n",
      "[*] Epoch: 0009 train_loss= 0.52013 train_acc= 0.74806 val_loss= 6.22837 val_acc= 0.50149 \t\ttime= 1.89411\n",
      "[*] Epoch: 0010 train_loss= 0.51571 train_acc= 0.75110 val_loss= 6.32167 val_acc= 0.50149 \t\ttime= 1.95723\n",
      "[*] Epoch: 0011 train_loss= 0.49687 train_acc= 0.76320 val_loss= 6.29264 val_acc= 0.50159 \t\ttime= 1.80465\n",
      "[*] Epoch: 0012 train_loss= 0.48785 train_acc= 0.76812 val_loss= 6.21245 val_acc= 0.50139 \t\ttime= 1.83478\n",
      "[*] Epoch: 0013 train_loss= 0.47720 train_acc= 0.77290 val_loss= 6.10633 val_acc= 0.50124 \t\ttime= 1.90913\n",
      "[*] Epoch: 0014 train_loss= 0.46812 train_acc= 0.78000 val_loss= 5.97878 val_acc= 0.50088 \t\ttime= 1.89661\n",
      "[*] Epoch: 0015 train_loss= 0.45755 train_acc= 0.78758 val_loss= 5.86067 val_acc= 0.50085 \t\ttime= 1.73849\n",
      "[*] Epoch: 0016 train_loss= 0.44305 train_acc= 0.79766 val_loss= 5.73649 val_acc= 0.50090 \t\ttime= 1.78690\n",
      "[*] Epoch: 0017 train_loss= 0.43961 train_acc= 0.80010 val_loss= 5.62722 val_acc= 0.50076 \t\ttime= 1.94952\n",
      "[*] Epoch: 0018 train_loss= 0.43296 train_acc= 0.80172 val_loss= 5.50298 val_acc= 0.50078 \t\ttime= 1.87718\n",
      "[*] Epoch: 0019 train_loss= 0.41864 train_acc= 0.80930 val_loss= 5.34570 val_acc= 0.50083 \t\ttime= 1.81342\n",
      "[*] Epoch: 0020 train_loss= 0.40868 train_acc= 0.81635 val_loss= 5.15404 val_acc= 0.50083 \t\ttime= 1.89810\n",
      "[*] Epoch: 0021 train_loss= 0.40643 train_acc= 0.81512 val_loss= 4.93611 val_acc= 0.50085 \t\ttime= 1.98393\n",
      "[*] Epoch: 0022 train_loss= 0.38826 train_acc= 0.82385 val_loss= 4.71306 val_acc= 0.50083 \t\ttime= 1.82512\n",
      "[*] Epoch: 0023 train_loss= 0.38048 train_acc= 0.83070 val_loss= 4.49576 val_acc= 0.50083 \t\ttime= 1.74578\n",
      "[*] Epoch: 0024 train_loss= 0.38140 train_acc= 0.82894 val_loss= 4.30548 val_acc= 0.50085 \t\ttime= 1.82181\n",
      "[*] Epoch: 0025 train_loss= 0.37291 train_acc= 0.83446 val_loss= 4.13072 val_acc= 0.50085 \t\ttime= 1.95367\n",
      "[*] Epoch: 0026 train_loss= 0.36302 train_acc= 0.83941 val_loss= 3.95866 val_acc= 0.50090 \t\ttime= 1.82194\n",
      "[*] Epoch: 0027 train_loss= 0.36007 train_acc= 0.84066 val_loss= 3.80051 val_acc= 0.50088 \t\ttime= 1.82370\n",
      "[*] Epoch: 0028 train_loss= 0.33662 train_acc= 0.85345 val_loss= 3.64641 val_acc= 0.50090 \t\ttime= 1.86090\n",
      "[*] Epoch: 0029 train_loss= 0.33542 train_acc= 0.85195 val_loss= 3.49381 val_acc= 0.50090 \t\ttime= 1.87258\n",
      "[*] Epoch: 0030 train_loss= 0.33093 train_acc= 0.85435 val_loss= 3.35088 val_acc= 0.50102 \t\ttime= 1.79236\n",
      "[*] Epoch: 0031 train_loss= 0.33155 train_acc= 0.85516 val_loss= 3.21542 val_acc= 0.50134 \t\ttime= 1.79218\n",
      "[*] Epoch: 0032 train_loss= 0.33040 train_acc= 0.85410 val_loss= 3.09582 val_acc= 0.50146 \t\ttime= 1.88403\n",
      "[*] Epoch: 0033 train_loss= 0.32222 train_acc= 0.85947 val_loss= 2.97420 val_acc= 0.50176 \t\ttime= 1.87881\n",
      "[*] Epoch: 0034 train_loss= 0.31848 train_acc= 0.86290 val_loss= 2.85810 val_acc= 0.50219 \t\ttime= 1.79418\n",
      "[*] Epoch: 0035 train_loss= 0.31307 train_acc= 0.86387 val_loss= 2.75776 val_acc= 0.50288 \t\ttime= 1.73628\n",
      "[*] Epoch: 0036 train_loss= 0.30100 train_acc= 0.87118 val_loss= 2.67720 val_acc= 0.50315 \t\ttime= 1.88380\n",
      "[*] Epoch: 0037 train_loss= 0.30090 train_acc= 0.87230 val_loss= 2.60066 val_acc= 0.50366 \t\ttime= 1.85528\n",
      "[*] Epoch: 0038 train_loss= 0.29583 train_acc= 0.87300 val_loss= 2.53760 val_acc= 0.50373 \t\ttime= 1.73952\n",
      "[*] Epoch: 0039 train_loss= 0.29284 train_acc= 0.87496 val_loss= 2.48124 val_acc= 0.50395 \t\ttime= 1.83740\n",
      "[*] Epoch: 0040 train_loss= 0.30034 train_acc= 0.87077 val_loss= 2.43176 val_acc= 0.50380 \t\ttime= 1.77439\n",
      "[*] Epoch: 0041 train_loss= 0.28681 train_acc= 0.87780 val_loss= 2.39066 val_acc= 0.50380 \t\ttime= 1.81882\n",
      "[*] Epoch: 0042 train_loss= 0.28135 train_acc= 0.88070 val_loss= 2.35583 val_acc= 0.50376 \t\ttime= 1.76971\n",
      "[*] Epoch: 0043 train_loss= 0.28005 train_acc= 0.88176 val_loss= 2.32243 val_acc= 0.50356 \t\ttime= 1.99507\n",
      "[*] Epoch: 0044 train_loss= 0.29045 train_acc= 0.87498 val_loss= 2.28769 val_acc= 0.50378 \t\ttime= 1.79386\n",
      "[*] Epoch: 0045 train_loss= 0.28611 train_acc= 0.87906 val_loss= 2.25115 val_acc= 0.50410 \t\ttime= 1.73507\n",
      "[*] Epoch: 0046 train_loss= 0.27748 train_acc= 0.88162 val_loss= 2.21153 val_acc= 0.50456 \t\ttime= 1.86477\n",
      "[*] Epoch: 0047 train_loss= 0.26991 train_acc= 0.88669 val_loss= 2.17135 val_acc= 0.50497 \t\ttime= 1.78344\n",
      "[*] Epoch: 0048 train_loss= 0.26885 train_acc= 0.88693 val_loss= 2.13513 val_acc= 0.50519 \t\ttime= 1.72609\n",
      "[*] Epoch: 0049 train_loss= 0.27763 train_acc= 0.88312 val_loss= 2.09729 val_acc= 0.50551 \t\ttime= 1.77176\n",
      "[*] Epoch: 0050 train_loss= 0.26705 train_acc= 0.88803 val_loss= 2.05816 val_acc= 0.50627 \t\ttime= 1.78743\n",
      "[*] Epoch: 0051 train_loss= 0.27413 train_acc= 0.88359 val_loss= 2.01996 val_acc= 0.50697 \t\ttime= 1.94595\n",
      "[*] Epoch: 0052 train_loss= 0.27662 train_acc= 0.88220 val_loss= 1.98096 val_acc= 0.50790 \t\ttime= 1.77406\n",
      "[*] Epoch: 0053 train_loss= 0.26387 train_acc= 0.88895 val_loss= 1.93953 val_acc= 0.50939 \t\ttime= 1.78225\n",
      "[*] Epoch: 0054 train_loss= 0.26477 train_acc= 0.88946 val_loss= 1.90290 val_acc= 0.50978 \t\ttime= 1.87271\n",
      "[*] Epoch: 0055 train_loss= 0.26127 train_acc= 0.89061 val_loss= 1.86784 val_acc= 0.51061 \t\ttime= 1.76764\n",
      "[*] Epoch: 0056 train_loss= 0.27195 train_acc= 0.88410 val_loss= 1.82919 val_acc= 0.51129 \t\ttime= 1.82448\n",
      "[*] Epoch: 0057 train_loss= 0.27187 train_acc= 0.88378 val_loss= 1.79383 val_acc= 0.51302 \t\ttime= 1.75923\n",
      "[*] Epoch: 0058 train_loss= 0.26283 train_acc= 0.88920 val_loss= 1.76208 val_acc= 0.51566 \t\ttime= 1.74526\n",
      "[*] Epoch: 0059 train_loss= 0.26136 train_acc= 0.88912 val_loss= 1.73234 val_acc= 0.51705 \t\ttime= 1.89805\n",
      "[*] Epoch: 0060 train_loss= 0.25948 train_acc= 0.89031 val_loss= 1.69969 val_acc= 0.51802 \t\ttime= 1.77418\n",
      "[*] Epoch: 0061 train_loss= 0.25558 train_acc= 0.89150 val_loss= 1.66650 val_acc= 0.51875 \t\ttime= 1.80068\n",
      "[*] Epoch: 0062 train_loss= 0.25620 train_acc= 0.89234 val_loss= 1.63659 val_acc= 0.52027 \t\ttime= 1.75511\n",
      "[*] Epoch: 0063 train_loss= 0.25191 train_acc= 0.89408 val_loss= 1.61217 val_acc= 0.52168 \t\ttime= 1.79035\n",
      "[*] Epoch: 0064 train_loss= 0.25195 train_acc= 0.89414 val_loss= 1.58895 val_acc= 0.52292 \t\ttime= 1.91394\n",
      "[*] Epoch: 0065 train_loss= 0.25874 train_acc= 0.89078 val_loss= 1.56021 val_acc= 0.52622 \t\ttime= 1.73898\n",
      "[*] Epoch: 0066 train_loss= 0.24715 train_acc= 0.89651 val_loss= 1.53188 val_acc= 0.52865 \t\ttime= 1.89740\n",
      "[*] Epoch: 0067 train_loss= 0.24667 train_acc= 0.89687 val_loss= 1.50415 val_acc= 0.53165 \t\ttime= 1.77772\n",
      "[*] Epoch: 0068 train_loss= 0.25422 train_acc= 0.89358 val_loss= 1.47892 val_acc= 0.53443 \t\ttime= 1.78200\n",
      "[*] Epoch: 0069 train_loss= 0.24547 train_acc= 0.89728 val_loss= 1.45644 val_acc= 0.53636 \t\ttime= 1.81607\n",
      "[*] Epoch: 0070 train_loss= 0.25284 train_acc= 0.89338 val_loss= 1.43503 val_acc= 0.53758 \t\ttime= 1.80153\n",
      "[*] Epoch: 0071 train_loss= 0.24825 train_acc= 0.89501 val_loss= 1.41306 val_acc= 0.53843 \t\ttime= 1.92390\n",
      "[*] Epoch: 0072 train_loss= 0.24360 train_acc= 0.89752 val_loss= 1.39239 val_acc= 0.54007 \t\ttime= 1.76679\n",
      "[*] Epoch: 0073 train_loss= 0.24317 train_acc= 0.89895 val_loss= 1.37185 val_acc= 0.54246 \t\ttime= 1.75600\n",
      "[*] Epoch: 0074 train_loss= 0.23937 train_acc= 0.90048 val_loss= 1.35227 val_acc= 0.54424 \t\ttime= 1.87004\n",
      "[*] Epoch: 0075 train_loss= 0.24246 train_acc= 0.89871 val_loss= 1.33478 val_acc= 0.54575 \t\ttime= 1.80820\n",
      "[*] Epoch: 0076 train_loss= 0.24378 train_acc= 0.89830 val_loss= 1.31989 val_acc= 0.54636 \t\ttime= 1.82968\n",
      "[*] Epoch: 0077 train_loss= 0.24573 train_acc= 0.89638 val_loss= 1.30511 val_acc= 0.54704 \t\ttime= 1.77232\n",
      "[*] Epoch: 0078 train_loss= 0.24059 train_acc= 0.89945 val_loss= 1.29357 val_acc= 0.54770 \t\ttime= 1.86157\n",
      "[*] Epoch: 0079 train_loss= 0.23770 train_acc= 0.90190 val_loss= 1.28156 val_acc= 0.54824 \t\ttime= 1.79852\n",
      "[*] Epoch: 0080 train_loss= 0.23761 train_acc= 0.90067 val_loss= 1.26360 val_acc= 0.55080 \t\ttime= 1.86618\n",
      "[*] Epoch: 0081 train_loss= 0.24688 train_acc= 0.89618 val_loss= 1.24126 val_acc= 0.55589 \t\ttime= 1.78488\n",
      "[*] Epoch: 0082 train_loss= 0.23898 train_acc= 0.90025 val_loss= 1.22232 val_acc= 0.55975 \t\ttime= 1.78974\n",
      "[*] Epoch: 0083 train_loss= 0.23269 train_acc= 0.90403 val_loss= 1.20582 val_acc= 0.56262 \t\ttime= 1.78513\n",
      "[*] Epoch: 0084 train_loss= 0.23753 train_acc= 0.90121 val_loss= 1.19014 val_acc= 0.56404 \t\ttime= 1.87222\n",
      "[*] Epoch: 0085 train_loss= 0.23898 train_acc= 0.90018 val_loss= 1.17944 val_acc= 0.56480 \t\ttime= 1.78475\n",
      "[*] Epoch: 0086 train_loss= 0.23295 train_acc= 0.90263 val_loss= 1.17382 val_acc= 0.56489 \t\ttime= 1.89779\n",
      "[*] Epoch: 0087 train_loss= 0.24120 train_acc= 0.89874 val_loss= 1.16514 val_acc= 0.56626 \t\ttime= 1.75218\n",
      "[*] Epoch: 0088 train_loss= 0.23042 train_acc= 0.90417 val_loss= 1.15899 val_acc= 0.56577 \t\ttime= 1.83698\n",
      "[*] Epoch: 0089 train_loss= 0.23777 train_acc= 0.90204 val_loss= 1.15150 val_acc= 0.56611 \t\ttime= 1.95692\n",
      "[*] Epoch: 0090 train_loss= 0.23153 train_acc= 0.90510 val_loss= 1.14234 val_acc= 0.56706 \t\ttime= 1.80043\n",
      "[*] Epoch: 0091 train_loss= 0.23292 train_acc= 0.90251 val_loss= 1.13155 val_acc= 0.56916 \t\ttime= 1.77661\n",
      "[*] Epoch: 0092 train_loss= 0.23011 train_acc= 0.90437 val_loss= 1.11807 val_acc= 0.57279 \t\ttime= 1.82712\n",
      "[*] Epoch: 0093 train_loss= 0.22641 train_acc= 0.90514 val_loss= 1.11065 val_acc= 0.57418 \t\ttime= 1.81482\n",
      "[*] Epoch: 0094 train_loss= 0.22683 train_acc= 0.90670 val_loss= 1.10807 val_acc= 0.57418 \t\ttime= 2.02137\n",
      "[*] Epoch: 0095 train_loss= 0.22849 train_acc= 0.90447 val_loss= 1.10081 val_acc= 0.57475 \t\ttime= 1.83323\n",
      "[*] Epoch: 0096 train_loss= 0.22774 train_acc= 0.90467 val_loss= 1.09017 val_acc= 0.57599 \t\ttime= 1.78382\n",
      "[*] Epoch: 0097 train_loss= 0.22844 train_acc= 0.90482 val_loss= 1.07606 val_acc= 0.57835 \t\ttime= 1.90897\n",
      "[*] Epoch: 0098 train_loss= 0.22963 train_acc= 0.90343 val_loss= 1.05903 val_acc= 0.58233 \t\ttime= 1.78709\n",
      "[*] Epoch: 0099 train_loss= 0.22682 train_acc= 0.90595 val_loss= 1.04048 val_acc= 0.58706 \t\ttime= 2.03438\n",
      "[*] Epoch: 0100 train_loss= 0.23147 train_acc= 0.90262 val_loss= 1.02254 val_acc= 0.59245 \t\ttime= 1.72877\n",
      "[*] Epoch: 0101 train_loss= 0.22665 train_acc= 0.90466 val_loss= 1.01649 val_acc= 0.59479 \t\ttime= 1.83868\n",
      "[*] Epoch: 0102 train_loss= 0.22571 train_acc= 0.90538 val_loss= 1.01251 val_acc= 0.59569 \t\ttime= 1.80250\n",
      "[*] Epoch: 0103 train_loss= 0.22335 train_acc= 0.90844 val_loss= 1.00261 val_acc= 0.59701 \t\ttime= 1.83374\n",
      "[*] Epoch: 0104 train_loss= 0.22598 train_acc= 0.90496 val_loss= 0.99049 val_acc= 0.59881 \t\ttime= 1.75042\n",
      "[*] Epoch: 0105 train_loss= 0.22476 train_acc= 0.90675 val_loss= 0.97591 val_acc= 0.60223 \t\ttime= 1.76855\n",
      "[*] Epoch: 0106 train_loss= 0.21923 train_acc= 0.90885 val_loss= 0.96332 val_acc= 0.60628 \t\ttime= 1.85302\n",
      "[*] Epoch: 0107 train_loss= 0.22353 train_acc= 0.90645 val_loss= 0.94531 val_acc= 0.61357 \t\ttime= 1.78681\n",
      "[*] Epoch: 0108 train_loss= 0.22316 train_acc= 0.90681 val_loss= 0.92519 val_acc= 0.62071 \t\ttime= 1.91888\n",
      "[*] Epoch: 0109 train_loss= 0.23361 train_acc= 0.90259 val_loss= 0.90575 val_acc= 0.62652 \t\ttime= 1.82679\n",
      "[*] Epoch: 0110 train_loss= 0.22642 train_acc= 0.90571 val_loss= 0.89523 val_acc= 0.62891 \t\ttime= 1.97597\n",
      "[*] Epoch: 0111 train_loss= 0.21623 train_acc= 0.91100 val_loss= 0.89393 val_acc= 0.62781 \t\ttime= 1.79889\n",
      "[*] Epoch: 0112 train_loss= 0.22482 train_acc= 0.90606 val_loss= 0.88981 val_acc= 0.63198 \t\ttime= 1.80189\n",
      "[*] Epoch: 0113 train_loss= 0.22420 train_acc= 0.90563 val_loss= 0.87924 val_acc= 0.63788 \t\ttime= 1.76835\n",
      "[*] Epoch: 0114 train_loss= 0.23059 train_acc= 0.90375 val_loss= 0.86526 val_acc= 0.64310 \t\ttime= 1.76503\n",
      "[*] Epoch: 0115 train_loss= 0.21534 train_acc= 0.91147 val_loss= 0.85548 val_acc= 0.64603 \t\ttime= 1.81946\n",
      "[*] Epoch: 0116 train_loss= 0.22042 train_acc= 0.90956 val_loss= 0.84863 val_acc= 0.64734 \t\ttime= 1.80297\n",
      "[*] Epoch: 0117 train_loss= 0.21784 train_acc= 0.91040 val_loss= 0.84969 val_acc= 0.64537 \t\ttime= 1.82779\n",
      "[*] Epoch: 0118 train_loss= 0.22109 train_acc= 0.90783 val_loss= 0.84737 val_acc= 0.64520 \t\ttime= 1.82490\n",
      "[*] Epoch: 0119 train_loss= 0.21472 train_acc= 0.91102 val_loss= 0.84280 val_acc= 0.64754 \t\ttime= 1.79383\n",
      "[*] Epoch: 0120 train_loss= 0.23105 train_acc= 0.90339 val_loss= 0.82772 val_acc= 0.65256 \t\ttime= 1.84362\n",
      "[*] Epoch: 0121 train_loss= 0.21405 train_acc= 0.91135 val_loss= 0.81148 val_acc= 0.65710 \t\ttime= 1.76812\n",
      "[*] Epoch: 0122 train_loss= 0.21629 train_acc= 0.91036 val_loss= 0.79747 val_acc= 0.66205 \t\ttime= 1.91025\n",
      "[*] Epoch: 0123 train_loss= 0.22170 train_acc= 0.90819 val_loss= 0.78759 val_acc= 0.66580 \t\ttime= 1.83269\n",
      "[*] Epoch: 0124 train_loss= 0.22175 train_acc= 0.90866 val_loss= 0.78176 val_acc= 0.66812 \t\ttime= 1.95793\n",
      "[*] Epoch: 0125 train_loss= 0.22309 train_acc= 0.90696 val_loss= 0.78146 val_acc= 0.66785 \t\ttime= 1.78587\n",
      "[*] Epoch: 0126 train_loss= 0.21457 train_acc= 0.91140 val_loss= 0.78816 val_acc= 0.66405 \t\ttime= 1.81296\n",
      "[*] Epoch: 0127 train_loss= 0.21637 train_acc= 0.91097 val_loss= 0.78837 val_acc= 0.66276 \t\ttime= 2.01670\n",
      "[*] Epoch: 0128 train_loss= 0.21724 train_acc= 0.90828 val_loss= 0.77741 val_acc= 0.66720 \t\ttime= 1.80469\n",
      "[*] Epoch: 0129 train_loss= 0.21811 train_acc= 0.90844 val_loss= 0.76167 val_acc= 0.67436 \t\ttime= 1.75664\n",
      "[*] Epoch: 0130 train_loss= 0.21901 train_acc= 0.90846 val_loss= 0.74111 val_acc= 0.68351 \t\ttime= 1.99747\n",
      "[*] Epoch: 0131 train_loss= 0.22338 train_acc= 0.90663 val_loss= 0.72412 val_acc= 0.69063 \t\ttime= 1.78728\n",
      "[*] Epoch: 0132 train_loss= 0.21461 train_acc= 0.91123 val_loss= 0.72201 val_acc= 0.69095 \t\ttime= 1.98997\n",
      "[*] Epoch: 0133 train_loss= 0.22422 train_acc= 0.90585 val_loss= 0.71887 val_acc= 0.69134 \t\ttime= 1.77640\n",
      "[*] Epoch: 0134 train_loss= 0.21781 train_acc= 0.90962 val_loss= 0.72204 val_acc= 0.68897 \t\ttime= 1.92916\n",
      "[*] Epoch: 0135 train_loss= 0.21694 train_acc= 0.90870 val_loss= 0.72888 val_acc= 0.68683 \t\ttime= 1.84049\n",
      "[*] Epoch: 0136 train_loss= 0.22158 train_acc= 0.90690 val_loss= 0.72057 val_acc= 0.69075 \t\ttime= 1.78935\n",
      "[*] Epoch: 0137 train_loss= 0.21598 train_acc= 0.90917 val_loss= 0.69908 val_acc= 0.69895 \t\ttime= 1.79047\n",
      "[*] Epoch: 0138 train_loss= 0.21403 train_acc= 0.91156 val_loss= 0.68169 val_acc= 0.70531 \t\ttime= 1.78204\n",
      "[*] Epoch: 0139 train_loss= 0.21460 train_acc= 0.90874 val_loss= 0.66715 val_acc= 0.70985 \t\ttime= 1.77830\n",
      "[*] Epoch: 0140 train_loss= 0.21691 train_acc= 0.90878 val_loss= 0.66179 val_acc= 0.71229 \t\ttime= 1.82101\n",
      "[*] Epoch: 0141 train_loss= 0.21941 train_acc= 0.90779 val_loss= 0.65485 val_acc= 0.71687 \t\ttime= 1.79043\n",
      "[*] Epoch: 0142 train_loss= 0.21627 train_acc= 0.90984 val_loss= 0.65201 val_acc= 0.71916 \t\ttime= 1.73827\n",
      "[*] Epoch: 0143 train_loss= 0.20911 train_acc= 0.91469 val_loss= 0.64629 val_acc= 0.72228 \t\ttime= 1.79023\n",
      "[*] Epoch: 0144 train_loss= 0.21201 train_acc= 0.91184 val_loss= 0.63712 val_acc= 0.72702 \t\ttime= 1.80226\n",
      "[*] Epoch: 0145 train_loss= 0.21629 train_acc= 0.91009 val_loss= 0.62155 val_acc= 0.73250 \t\ttime= 1.83652\n",
      "[*] Epoch: 0146 train_loss= 0.21394 train_acc= 0.91044 val_loss= 0.61116 val_acc= 0.73675 \t\ttime= 1.77548\n",
      "[*] Epoch: 0147 train_loss= 0.21354 train_acc= 0.91084 val_loss= 0.60845 val_acc= 0.73845 \t\ttime= 1.94424\n",
      "[*] Epoch: 0148 train_loss= 0.21380 train_acc= 0.91120 val_loss= 0.61352 val_acc= 0.73760 \t\ttime= 1.81144\n",
      "[*] Epoch: 0149 train_loss= 0.20450 train_acc= 0.91658 val_loss= 0.62565 val_acc= 0.73204 \t\ttime= 1.77530\n",
      "[*] Epoch: 0150 train_loss= 0.21387 train_acc= 0.91118 val_loss= 0.62983 val_acc= 0.73019 \t\ttime= 1.91944\n",
      "[*] Epoch: 0151 train_loss= 0.21701 train_acc= 0.90956 val_loss= 0.61817 val_acc= 0.73484 \t\ttime= 1.84393\n",
      "[*] Epoch: 0152 train_loss= 0.21809 train_acc= 0.90840 val_loss= 0.60970 val_acc= 0.73797 \t\ttime= 1.86079\n",
      "[*] Epoch: 0153 train_loss= 0.22119 train_acc= 0.90745 val_loss= 0.60800 val_acc= 0.73877 \t\ttime= 1.80529\n",
      "[*] Epoch: 0154 train_loss= 0.21289 train_acc= 0.91183 val_loss= 0.61501 val_acc= 0.73582 \t\ttime= 1.78580\n",
      "[*] Epoch: 0155 train_loss= 0.20505 train_acc= 0.91550 val_loss= 0.62724 val_acc= 0.73036 \t\ttime= 1.81118\n",
      "[*] Epoch: 0156 train_loss= 0.21502 train_acc= 0.91015 val_loss= 0.63409 val_acc= 0.72806 \t\ttime= 1.87193\n",
      "[*] Epoch: 0157 train_loss= 0.20470 train_acc= 0.91569 val_loss= 0.63179 val_acc= 0.72855 \t\ttime= 1.97201\n",
      "[*] Epoch: 0158 train_loss= 0.20759 train_acc= 0.91348 val_loss= 0.61970 val_acc= 0.73258 \t\ttime= 1.81678\n",
      "[*] Epoch: 0159 train_loss= 0.21603 train_acc= 0.90848 val_loss= 0.60504 val_acc= 0.73775 \t\ttime= 1.79096\n",
      "[*] Epoch: 0160 train_loss= 0.21057 train_acc= 0.91226 val_loss= 0.59519 val_acc= 0.74196 \t\ttime= 1.90990\n",
      "[*] Epoch: 0161 train_loss= 0.20616 train_acc= 0.91457 val_loss= 0.59328 val_acc= 0.74274 \t\ttime= 1.74400\n",
      "[*] Epoch: 0162 train_loss= 0.20865 train_acc= 0.91387 val_loss= 0.59712 val_acc= 0.74023 \t\ttime= 1.92824\n",
      "[*] Epoch: 0163 train_loss= 0.20628 train_acc= 0.91538 val_loss= 0.60062 val_acc= 0.73831 \t\ttime= 1.87332\n",
      "[*] Epoch: 0164 train_loss= 0.21115 train_acc= 0.91125 val_loss= 0.60158 val_acc= 0.73823 \t\ttime= 1.81261\n",
      "[*] Epoch: 0165 train_loss= 0.21416 train_acc= 0.91019 val_loss= 0.59278 val_acc= 0.74216 \t\ttime= 1.81126\n",
      "[*] Epoch: 0166 train_loss= 0.21026 train_acc= 0.91210 val_loss= 0.58142 val_acc= 0.74821 \t\ttime= 2.04363\n",
      "[*] Epoch: 0167 train_loss= 0.20790 train_acc= 0.91419 val_loss= 0.56674 val_acc= 0.75452 \t\ttime= 1.80521\n",
      "[*] Epoch: 0168 train_loss= 0.21175 train_acc= 0.91177 val_loss= 0.55147 val_acc= 0.76208 \t\ttime= 1.91228\n",
      "[*] Epoch: 0169 train_loss= 0.22168 train_acc= 0.90663 val_loss= 0.54078 val_acc= 0.76730 \t\ttime= 1.85786\n",
      "[*] Epoch: 0170 train_loss= 0.20760 train_acc= 0.91368 val_loss= 0.53827 val_acc= 0.76859 \t\ttime= 1.88404\n",
      "[*] Epoch: 0171 train_loss= 0.21201 train_acc= 0.91120 val_loss= 0.53940 val_acc= 0.76820 \t\ttime= 1.83351\n",
      "[*] Epoch: 0172 train_loss= 0.19919 train_acc= 0.91768 val_loss= 0.54338 val_acc= 0.76642 \t\ttime= 1.94808\n",
      "[*] Epoch: 0173 train_loss= 0.20248 train_acc= 0.91648 val_loss= 0.55080 val_acc= 0.76286 \t\ttime= 1.88306\n",
      "[*] Epoch: 0174 train_loss= 0.20880 train_acc= 0.91379 val_loss= 0.54830 val_acc= 0.76408 \t\ttime= 1.83335\n",
      "[*] Epoch: 0175 train_loss= 0.20862 train_acc= 0.91273 val_loss= 0.53687 val_acc= 0.76833 \t\ttime= 1.82390\n",
      "[*] Epoch: 0176 train_loss= 0.20521 train_acc= 0.91497 val_loss= 0.52450 val_acc= 0.77311 \t\ttime= 1.94897\n",
      "[*] Epoch: 0177 train_loss= 0.21065 train_acc= 0.91267 val_loss= 0.51751 val_acc= 0.77711 \t\ttime= 1.81339\n",
      "[*] Epoch: 0178 train_loss= 0.20735 train_acc= 0.91274 val_loss= 0.51896 val_acc= 0.77618 \t\ttime= 1.93119\n",
      "[*] Epoch: 0179 train_loss= 0.20823 train_acc= 0.91353 val_loss= 0.52565 val_acc= 0.77316 \t\ttime= 1.87499\n",
      "[*] Epoch: 0180 train_loss= 0.20438 train_acc= 0.91426 val_loss= 0.53617 val_acc= 0.76952 \t\ttime= 1.88883\n",
      "[*] Epoch: 0181 train_loss= 0.20694 train_acc= 0.91383 val_loss= 0.53954 val_acc= 0.76808 \t\ttime= 1.94778\n",
      "[*] Epoch: 0182 train_loss= 0.20900 train_acc= 0.91331 val_loss= 0.53822 val_acc= 0.76911 \t\ttime= 2.05683\n",
      "[*] Epoch: 0183 train_loss= 0.20590 train_acc= 0.91497 val_loss= 0.53182 val_acc= 0.77289 \t\ttime= 1.80754\n",
      "[*] Epoch: 0184 train_loss= 0.20451 train_acc= 0.91493 val_loss= 0.52260 val_acc= 0.77718 \t\ttime= 1.81261\n",
      "[*] Epoch: 0185 train_loss= 0.20219 train_acc= 0.91673 val_loss= 0.51528 val_acc= 0.78071 \t\ttime= 1.97383\n",
      "[*] Epoch: 0186 train_loss= 0.19928 train_acc= 0.91786 val_loss= 0.51411 val_acc= 0.78079 \t\ttime= 1.77086\n",
      "[*] Epoch: 0187 train_loss= 0.20594 train_acc= 0.91380 val_loss= 0.51844 val_acc= 0.77803 \t\ttime= 2.05401\n",
      "[*] Epoch: 0188 train_loss= 0.20455 train_acc= 0.91543 val_loss= 0.52186 val_acc= 0.77615 \t\ttime= 1.79696\n",
      "[*] Epoch: 0189 train_loss= 0.19725 train_acc= 0.91805 val_loss= 0.51980 val_acc= 0.77689 \t\ttime= 1.90944\n",
      "[*] Epoch: 0190 train_loss= 0.20358 train_acc= 0.91605 val_loss= 0.51206 val_acc= 0.78054 \t\ttime= 2.13019\n",
      "[*] Epoch: 0191 train_loss= 0.21132 train_acc= 0.91068 val_loss= 0.49568 val_acc= 0.78820 \t\ttime= 2.04433\n",
      "[*] Epoch: 0192 train_loss= 0.19833 train_acc= 0.91797 val_loss= 0.48452 val_acc= 0.79364 \t\ttime= 1.88000\n",
      "[*] Epoch: 0193 train_loss= 0.20176 train_acc= 0.91545 val_loss= 0.48077 val_acc= 0.79549 \t\ttime= 1.96697\n",
      "[*] Epoch: 0194 train_loss= 0.20120 train_acc= 0.91634 val_loss= 0.48280 val_acc= 0.79437 \t\ttime= 1.93629\n",
      "[*] Epoch: 0195 train_loss= 0.20271 train_acc= 0.91557 val_loss= 0.48030 val_acc= 0.79586 \t\ttime= 2.23584\n",
      "[*] Epoch: 0196 train_loss= 0.20426 train_acc= 0.91390 val_loss= 0.47793 val_acc= 0.79693 \t\ttime= 1.96434\n",
      "[*] Epoch: 0197 train_loss= 0.20241 train_acc= 0.91633 val_loss= 0.47680 val_acc= 0.79627 \t\ttime= 2.06689\n",
      "[*] Epoch: 0198 train_loss= 0.20391 train_acc= 0.91507 val_loss= 0.47176 val_acc= 0.79857 \t\ttime= 1.80322\n",
      "[*] Epoch: 0199 train_loss= 0.20858 train_acc= 0.91337 val_loss= 0.46363 val_acc= 0.80210 \t\ttime= 1.82487\n",
      "[*] Epoch: 0200 train_loss= 0.20116 train_acc= 0.91745 val_loss= 0.45985 val_acc= 0.80378 \t\ttime= 2.05410\n",
      "[*] Epoch: 0201 train_loss= 0.20228 train_acc= 0.91633 val_loss= 0.46013 val_acc= 0.80288 \t\ttime= 1.97825\n",
      "[*] Epoch: 0202 train_loss= 0.20614 train_acc= 0.91390 val_loss= 0.46195 val_acc= 0.80198 \t\ttime= 1.88677\n",
      "[*] Epoch: 0203 train_loss= 0.20525 train_acc= 0.91376 val_loss= 0.46433 val_acc= 0.80113 \t\ttime= 1.83038\n",
      "[*] Epoch: 0204 train_loss= 0.20072 train_acc= 0.91698 val_loss= 0.46490 val_acc= 0.80137 \t\ttime= 1.81350\n",
      "[*] Epoch: 0205 train_loss= 0.19813 train_acc= 0.91894 val_loss= 0.46492 val_acc= 0.80115 \t\ttime= 2.00757\n",
      "[*] Epoch: 0206 train_loss= 0.20076 train_acc= 0.91736 val_loss= 0.46376 val_acc= 0.80183 \t\ttime= 1.82634\n",
      "[*] Epoch: 0207 train_loss= 0.20200 train_acc= 0.91519 val_loss= 0.45963 val_acc= 0.80359 \t\ttime= 1.85227\n",
      "[*] Epoch: 0208 train_loss= 0.19787 train_acc= 0.91835 val_loss= 0.45685 val_acc= 0.80488 \t\ttime= 1.80921\n",
      "[*] Epoch: 0209 train_loss= 0.19360 train_acc= 0.92088 val_loss= 0.45303 val_acc= 0.80717 \t\ttime= 1.87410\n",
      "[*] Epoch: 0210 train_loss= 0.19842 train_acc= 0.91718 val_loss= 0.45212 val_acc= 0.80720 \t\ttime= 1.97743\n",
      "[*] Epoch: 0211 train_loss= 0.19601 train_acc= 0.91978 val_loss= 0.45279 val_acc= 0.80632 \t\ttime= 1.80463\n",
      "[*] Epoch: 0212 train_loss= 0.20483 train_acc= 0.91380 val_loss= 0.45060 val_acc= 0.80698 \t\ttime= 1.82627\n",
      "[*] Epoch: 0213 train_loss= 0.19326 train_acc= 0.92138 val_loss= 0.45220 val_acc= 0.80620 \t\ttime= 2.08001\n",
      "[*] Epoch: 0214 train_loss= 0.20093 train_acc= 0.91750 val_loss= 0.45147 val_acc= 0.80730 \t\ttime= 1.95705\n",
      "[*] Epoch: 0215 train_loss= 0.20722 train_acc= 0.91369 val_loss= 0.44416 val_acc= 0.81008 \t\ttime= 1.75108\n",
      "[*] Epoch: 0216 train_loss= 0.20131 train_acc= 0.91699 val_loss= 0.44146 val_acc= 0.81081 \t\ttime= 1.74947\n",
      "[*] Epoch: 0217 train_loss= 0.19697 train_acc= 0.91935 val_loss= 0.44342 val_acc= 0.80988 \t\ttime= 1.87489\n",
      "[*] Epoch: 0218 train_loss= 0.19189 train_acc= 0.92064 val_loss= 0.45456 val_acc= 0.80457 \t\ttime= 1.73888\n",
      "[*] Epoch: 0219 train_loss= 0.20152 train_acc= 0.91648 val_loss= 0.45879 val_acc= 0.80186 \t\ttime= 1.67902\n",
      "[*] Epoch: 0220 train_loss= 0.19879 train_acc= 0.91660 val_loss= 0.44915 val_acc= 0.80557 \t\ttime= 1.69389\n",
      "[*] Epoch: 0221 train_loss= 0.20187 train_acc= 0.91640 val_loss= 0.43499 val_acc= 0.81276 \t\ttime= 1.87906\n",
      "[*] Epoch: 0222 train_loss= 0.19627 train_acc= 0.91890 val_loss= 0.42557 val_acc= 0.81778 \t\ttime= 1.70556\n",
      "[*] Epoch: 0223 train_loss= 0.20246 train_acc= 0.91589 val_loss= 0.42203 val_acc= 0.81976 \t\ttime= 1.86224\n",
      "[*] Epoch: 0224 train_loss= 0.19415 train_acc= 0.91987 val_loss= 0.42806 val_acc= 0.81776 \t\ttime= 1.72934\n",
      "[*] Epoch: 0225 train_loss= 0.19614 train_acc= 0.91819 val_loss= 0.43411 val_acc= 0.81439 \t\ttime= 1.71596\n",
      "[*] Epoch: 0226 train_loss= 0.20007 train_acc= 0.91774 val_loss= 0.43711 val_acc= 0.81264 \t\ttime= 1.81299\n",
      "[*] Epoch: 0227 train_loss= 0.19230 train_acc= 0.92027 val_loss= 0.43943 val_acc= 0.81183 \t\ttime= 1.72212\n",
      "[*] Epoch: 0228 train_loss= 0.19587 train_acc= 0.91872 val_loss= 0.43693 val_acc= 0.81286 \t\ttime= 1.69750\n",
      "[*] Epoch: 0229 train_loss= 0.19776 train_acc= 0.91851 val_loss= 0.43178 val_acc= 0.81571 \t\ttime= 1.71515\n",
      "[*] Epoch: 0230 train_loss= 0.20323 train_acc= 0.91540 val_loss= 0.42509 val_acc= 0.81986 \t\ttime= 1.84625\n",
      "[*] Epoch: 0231 train_loss= 0.20139 train_acc= 0.91638 val_loss= 0.42123 val_acc= 0.82171 \t\ttime= 1.73081\n",
      "[*] Epoch: 0232 train_loss= 0.20090 train_acc= 0.91597 val_loss= 0.42295 val_acc= 0.82076 \t\ttime= 1.88707\n",
      "[*] Epoch: 0233 train_loss= 0.19568 train_acc= 0.91921 val_loss= 0.43338 val_acc= 0.81622 \t\ttime= 1.68733\n",
      "[*] Epoch: 0234 train_loss= 0.20125 train_acc= 0.91590 val_loss= 0.44335 val_acc= 0.81252 \t\ttime= 1.71770\n",
      "[*] Epoch: 0235 train_loss= 0.19744 train_acc= 0.91870 val_loss= 0.44780 val_acc= 0.81064 \t\ttime= 1.67528\n",
      "[*] Epoch: 0236 train_loss= 0.19466 train_acc= 0.91957 val_loss= 0.43852 val_acc= 0.81454 \t\ttime= 1.89853\n",
      "[*] Epoch: 0237 train_loss= 0.19631 train_acc= 0.91908 val_loss= 0.41913 val_acc= 0.82329 \t\ttime= 1.76543\n",
      "[*] Epoch: 0238 train_loss= 0.19557 train_acc= 0.91925 val_loss= 0.39975 val_acc= 0.83122 \t\ttime= 1.72294\n",
      "[*] Epoch: 0239 train_loss= 0.19959 train_acc= 0.91761 val_loss= 0.38848 val_acc= 0.83568 \t\ttime= 1.70733\n",
      "[*] Epoch: 0240 train_loss= 0.19597 train_acc= 0.91968 val_loss= 0.39143 val_acc= 0.83437 \t\ttime= 1.89694\n",
      "[*] Epoch: 0241 train_loss= 0.19278 train_acc= 0.92216 val_loss= 0.40463 val_acc= 0.82851 \t\ttime= 1.81353\n",
      "[*] Epoch: 0242 train_loss= 0.19540 train_acc= 0.91855 val_loss= 0.41824 val_acc= 0.82183 \t\ttime= 1.77846\n",
      "[*] Epoch: 0243 train_loss= 0.20192 train_acc= 0.91671 val_loss= 0.42287 val_acc= 0.82095 \t\ttime= 2.18607\n",
      "[*] Epoch: 0244 train_loss= 0.19755 train_acc= 0.91900 val_loss= 0.41307 val_acc= 0.82546 \t\ttime= 1.79059\n",
      "[*] Epoch: 0245 train_loss= 0.19568 train_acc= 0.91972 val_loss= 0.40006 val_acc= 0.83266 \t\ttime= 1.77426\n",
      "[*] Epoch: 0246 train_loss= 0.19347 train_acc= 0.92028 val_loss= 0.39768 val_acc= 0.83395 \t\ttime= 1.80543\n",
      "[*] Epoch: 0247 train_loss= 0.19764 train_acc= 0.91786 val_loss= 0.40178 val_acc= 0.83273 \t\ttime= 1.82628\n",
      "[*] Epoch: 0248 train_loss= 0.19825 train_acc= 0.91768 val_loss= 0.40781 val_acc= 0.82966 \t\ttime= 1.73689\n",
      "[*] Epoch: 0249 train_loss= 0.19859 train_acc= 0.91683 val_loss= 0.41188 val_acc= 0.82671 \t\ttime= 1.67264\n",
      "[*] Epoch: 0250 train_loss= 0.19106 train_acc= 0.92111 val_loss= 0.41127 val_acc= 0.82773 \t\ttime= 1.82591\n",
      "[*] Epoch: 0251 train_loss= 0.19490 train_acc= 0.91946 val_loss= 0.40254 val_acc= 0.83124 \t\ttime= 1.79086\n",
      "[*] Epoch: 0252 train_loss= 0.19421 train_acc= 0.91970 val_loss= 0.39247 val_acc= 0.83510 \t\ttime= 1.70461\n",
      "[*] Epoch: 0253 train_loss= 0.19405 train_acc= 0.91881 val_loss= 0.38782 val_acc= 0.83702 \t\ttime= 1.69668\n",
      "[*] Epoch: 0254 train_loss= 0.20061 train_acc= 0.91608 val_loss= 0.38853 val_acc= 0.83617 \t\ttime= 1.79643\n",
      "[*] Epoch: 0255 train_loss= 0.19268 train_acc= 0.92046 val_loss= 0.39362 val_acc= 0.83373 \t\ttime= 1.71338\n",
      "[*] Epoch: 0256 train_loss= 0.19568 train_acc= 0.91883 val_loss= 0.39878 val_acc= 0.83127 \t\ttime= 1.66967\n",
      "[*] Epoch: 0257 train_loss= 0.19537 train_acc= 0.91909 val_loss= 0.40184 val_acc= 0.83002 \t\ttime= 1.93062\n",
      "[*] Epoch: 0258 train_loss= 0.19768 train_acc= 0.91708 val_loss= 0.39990 val_acc= 0.83132 \t\ttime= 1.73189\n",
      "[*] Epoch: 0259 train_loss= 0.19083 train_acc= 0.92140 val_loss= 0.39882 val_acc= 0.83193 \t\ttime= 1.73753\n",
      "[*] Epoch: 0260 train_loss= 0.19261 train_acc= 0.91977 val_loss= 0.39818 val_acc= 0.83161 \t\ttime= 1.67998\n",
      "[*] Epoch: 0261 train_loss= 0.19858 train_acc= 0.91759 val_loss= 0.39684 val_acc= 0.83198 \t\ttime= 1.87526\n",
      "[*] Epoch: 0262 train_loss= 0.18960 train_acc= 0.92256 val_loss= 0.39618 val_acc= 0.83237 \t\ttime= 1.75442\n",
      "[*] Epoch: 0263 train_loss= 0.19581 train_acc= 0.91884 val_loss= 0.39495 val_acc= 0.83237 \t\ttime= 1.72801\n",
      "[*] Epoch: 0264 train_loss= 0.19886 train_acc= 0.91655 val_loss= 0.39431 val_acc= 0.83307 \t\ttime= 1.61818\n",
      "[*] Epoch: 0265 train_loss= 0.19241 train_acc= 0.92019 val_loss= 0.39444 val_acc= 0.83290 \t\ttime= 1.96238\n",
      "[*] Epoch: 0266 train_loss= 0.19001 train_acc= 0.92168 val_loss= 0.39661 val_acc= 0.83166 \t\ttime= 1.72305\n",
      "[*] Epoch: 0267 train_loss= 0.19364 train_acc= 0.92017 val_loss= 0.39950 val_acc= 0.83110 \t\ttime= 1.76146\n",
      "[*] Epoch: 0268 train_loss= 0.19041 train_acc= 0.92071 val_loss= 0.39929 val_acc= 0.83073 \t\ttime= 1.71697\n",
      "[*] Epoch: 0269 train_loss= 0.19539 train_acc= 0.91889 val_loss= 0.39439 val_acc= 0.83181 \t\ttime= 1.85220\n",
      "[*] Epoch: 0270 train_loss= 0.19120 train_acc= 0.92118 val_loss= 0.38817 val_acc= 0.83405 \t\ttime= 1.72004\n",
      "[*] Epoch: 0271 train_loss= 0.19120 train_acc= 0.92121 val_loss= 0.38567 val_acc= 0.83446 \t\ttime= 1.74209\n",
      "[*] Epoch: 0272 train_loss= 0.19563 train_acc= 0.91836 val_loss= 0.38109 val_acc= 0.83680 \t\ttime= 1.76255\n",
      "[*] Epoch: 0273 train_loss= 0.20126 train_acc= 0.91585 val_loss= 0.37847 val_acc= 0.83897 \t\ttime= 1.80932\n",
      "[*] Epoch: 0274 train_loss= 0.19392 train_acc= 0.92098 val_loss= 0.37856 val_acc= 0.83929 \t\ttime= 1.72919\n",
      "[*] Epoch: 0275 train_loss= 0.19286 train_acc= 0.92069 val_loss= 0.38282 val_acc= 0.83832 \t\ttime= 1.91018\n",
      "[*] Epoch: 0276 train_loss= 0.19625 train_acc= 0.91886 val_loss= 0.38683 val_acc= 0.83698 \t\ttime= 1.72658\n",
      "[*] Epoch: 0277 train_loss= 0.18943 train_acc= 0.92281 val_loss= 0.39170 val_acc= 0.83441 \t\ttime= 1.70605\n",
      "[*] Epoch: 0278 train_loss= 0.18975 train_acc= 0.92217 val_loss= 0.39253 val_acc= 0.83449 \t\ttime= 1.71391\n",
      "[*] Epoch: 0279 train_loss= 0.18979 train_acc= 0.92116 val_loss= 0.38890 val_acc= 0.83637 \t\ttime= 1.84722\n",
      "[*] Epoch: 0280 train_loss= 0.18485 train_acc= 0.92468 val_loss= 0.38694 val_acc= 0.83746 \t\ttime= 1.72711\n",
      "[*] Epoch: 0281 train_loss= 0.19279 train_acc= 0.92009 val_loss= 0.38062 val_acc= 0.84010 \t\ttime= 1.76239\n",
      "[*] Epoch: 0282 train_loss= 0.19543 train_acc= 0.91898 val_loss= 0.37982 val_acc= 0.83971 \t\ttime= 1.84786\n",
      "[*] Epoch: 0283 train_loss= 0.18861 train_acc= 0.92309 val_loss= 0.38473 val_acc= 0.83722 \t\ttime= 1.73577\n",
      "[*] Epoch: 0284 train_loss= 0.19252 train_acc= 0.91989 val_loss= 0.39066 val_acc= 0.83429 \t\ttime= 1.75125\n",
      "[*] Epoch: 0285 train_loss= 0.19219 train_acc= 0.91990 val_loss= 0.39460 val_acc= 0.83363 \t\ttime= 1.77208\n",
      "[*] Epoch: 0286 train_loss= 0.19450 train_acc= 0.91909 val_loss= 0.38998 val_acc= 0.83580 \t\ttime= 1.75342\n",
      "[*] Epoch: 0287 train_loss= 0.19344 train_acc= 0.91981 val_loss= 0.38263 val_acc= 0.83985 \t\ttime= 1.74829\n",
      "[*] Epoch: 0288 train_loss= 0.19123 train_acc= 0.92127 val_loss= 0.37584 val_acc= 0.84246 \t\ttime= 1.72097\n",
      "[*] Epoch: 0289 train_loss= 0.19349 train_acc= 0.92039 val_loss= 0.37533 val_acc= 0.84295 \t\ttime= 1.70930\n",
      "[*] Epoch: 0290 train_loss= 0.19187 train_acc= 0.92113 val_loss= 0.37842 val_acc= 0.84078 \t\ttime= 1.76438\n",
      "[*] Epoch: 0291 train_loss= 0.19063 train_acc= 0.92260 val_loss= 0.38175 val_acc= 0.83868 \t\ttime= 1.84879\n",
      "[*] Epoch: 0292 train_loss= 0.18951 train_acc= 0.92182 val_loss= 0.38522 val_acc= 0.83722 \t\ttime= 1.72794\n",
      "[*] Epoch: 0293 train_loss= 0.19075 train_acc= 0.92108 val_loss= 0.38421 val_acc= 0.83727 \t\ttime= 1.76350\n",
      "[*] Epoch: 0294 train_loss= 0.19382 train_acc= 0.91986 val_loss= 0.37694 val_acc= 0.84058 \t\ttime= 1.78078\n",
      "[*] Epoch: 0295 train_loss= 0.18964 train_acc= 0.92209 val_loss= 0.37241 val_acc= 0.84239 \t\ttime= 1.88993\n",
      "[*] Epoch: 0296 train_loss= 0.19051 train_acc= 0.92094 val_loss= 0.37191 val_acc= 0.84258 \t\ttime= 1.68616\n",
      "[*] Epoch: 0297 train_loss= 0.19011 train_acc= 0.92206 val_loss= 0.37401 val_acc= 0.84234 \t\ttime= 1.75147\n",
      "[*] Epoch: 0298 train_loss= 0.19350 train_acc= 0.92068 val_loss= 0.37858 val_acc= 0.84044 \t\ttime= 1.93776\n",
      "[*] Epoch: 0299 train_loss= 0.18774 train_acc= 0.92273 val_loss= 0.38496 val_acc= 0.83712 \t\ttime= 1.95577\n",
      "[*] Epoch: 0300 train_loss= 0.18663 train_acc= 0.92383 val_loss= 0.38797 val_acc= 0.83622 \t\ttime= 1.85803\n",
      "\n",
      "Optimization Finished!\n",
      "best validation score = 0.8429498 at iteration 288, with a train_score of 0.9203885793685913\n",
      "\n",
      "SETTINGS:\n",
      "\n",
      "amz_data Men_also_bought\n",
      "batch_norm True\n",
      "dataset amazon\n",
      "degree 1\n",
      "dropout 0.5\n",
      "epochs 300\n",
      "hidden [256, 128, 64]\n",
      "learning_rate 0.01\n",
      "summaries_dir D:/School/FARS/Dataset/Men/Men_also_bought/logs\n",
      "support_dropout 0.15\n",
      "weight_decay 0.0\n",
      "write_summary False\n",
      "global seed =  1691908756\n",
      "{\"dataset\": \"amazon\", \"learning_rate\": 0.01, \"weight_decay\": 0.0, \"epochs\": 300, \"hidden\": [256, 128, 64], \"dropout\": 0.5, \"degree\": 1, \"summaries_dir\": \"D:/School/FARS/Dataset/Men/Men_also_bought/logs\", \"support_dropout\": 0.15, \"write_summary\": false, \"batch_norm\": true, \"amz_data\": \"Men_also_bought\", \"best_val_score\": 0.8429498076438904, \"best_epoch\": 288, \"best_epoch_train_score\": 0.9203885793685913, \"best_train_score\": 0.9246774911880493, \"seed\": 1691908756}\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# Set random seed\n",
    "seed = int(time.time()) # 12342\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "ap = argparse.ArgumentParser(\"\")\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=\"amazon\",\n",
    "                choices=['amazon'],\n",
    "                help=\"Dataset string.\")\n",
    "\n",
    "ap.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.01,\n",
    "                help=\"Learning rate\")\n",
    "\n",
    "ap.add_argument(\"-wd\", \"--weight_decay\", type=float, default=0.,\n",
    "                help=\"Learning rate\")\n",
    "\n",
    "ap.add_argument(\"-e\", \"--epochs\", type=int, default=300,\n",
    "                help=\"Number training epochs\")\n",
    "\n",
    "ap.add_argument(\"-hi\", \"--hidden\", type=int, nargs='+', default=[256,128,64],\n",
    "                help=\"Number hidden units in the GCN layers.\")\n",
    "\n",
    "ap.add_argument(\"-do\", \"--dropout\", type=float, default=0.5,\n",
    "                help=\"Dropout fraction\")\n",
    "\n",
    "ap.add_argument(\"-deg\", \"--degree\", type=int, default=1,\n",
    "                help=\"Degree of the convolution (Number of supports)\")\n",
    "\n",
    "ap.add_argument(\"-sdir\", \"--summaries_dir\", type=str, default=\"D:/School/FARS/Dataset/\"+cat_name+\"/\"+subcat_name+\"/logs\",\n",
    "                help=\"Directory for saving tensorflow summaries.\")\n",
    "\n",
    "ap.add_argument(\"-sup_do\", \"--support_dropout\", type=float, default=0.15,\n",
    "                help=\"Use dropout on the support matrices, dropping all the connections from some nodes\")\n",
    "\n",
    "ap.add_argument('-ws', '--write_summary', dest='write_summary', default=False,\n",
    "                help=\"Option to turn on summary writing\", action='store_true')\n",
    "\n",
    "fp = ap.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-bn', '--batch_norm', dest='batch_norm',\n",
    "                help=\"Option to turn on batchnorm in GCN layers\", action='store_true')\n",
    "fp.add_argument('-no_bn', '--no_batch_norm', dest='batch_norm',\n",
    "                help=\"Option to turn off batchnorm\", action='store_false')\n",
    "ap.set_defaults(batch_norm=True)\n",
    "\n",
    "ap.add_argument(\"-amzd\", \"--amz_data\", type=str, default=subcat_name,\n",
    "            choices=[subcat_name],\n",
    "            help=\"Dataset string.\")\n",
    "\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "print('Settings:')\n",
    "print(args, '\\n')\n",
    "\n",
    "# Define parameters\n",
    "DATASET = args['dataset']\n",
    "NB_EPOCH = args['epochs']\n",
    "DO = args['dropout']\n",
    "HIDDEN = args['hidden']\n",
    "LR = args['learning_rate']\n",
    "WRITESUMMARY = args['write_summary']\n",
    "SUMMARIESDIR = args['summaries_dir']\n",
    "FEATURES = \"img\"\n",
    "NUMCLASSES = 2\n",
    "DEGREE = args['degree']\n",
    "BATCH_NORM = args['batch_norm']\n",
    "BN_AS_TRAIN = False\n",
    "SUP_DO = args['support_dropout']\n",
    "ADJ_SELF_CONNECTIONS = True\n",
    "VERBOSE = True\n",
    "\n",
    "# prepare data_loader\n",
    "if DATASET == 'amazon':\n",
    "    cat_rel = args['amz_data']\n",
    "    dl = DataLoaderAmazon(cat_rel=cat_rel)\n",
    "    train_features, adj_train, train_labels, train_r_indices, train_c_indices = dl.get_phase('train')\n",
    "    _, adj_val, val_labels, val_r_indices, val_c_indices = dl.get_phase('valid')\n",
    "    _, adj_test, test_labels, test_r_indices, test_c_indices = dl.get_phase('test')\n",
    "    train_features, mean, std = dl.normalize_features(train_features, get_moments=True)\n",
    "else:\n",
    "    raise NotImplementedError('A data loader for dataset {} does not exist'.format(DATASET))\n",
    "\n",
    "if not os.path.exists(SUMMARIESDIR):\n",
    "    os.makedirs(SUMMARIESDIR)\n",
    "\n",
    "if SUMMARIESDIR == 'logs/':\n",
    "    SUMMARIESDIR += str(len(os.listdir(SUMMARIESDIR)))\n",
    "\n",
    "log_file = SUMMARIESDIR + '/log.json'\n",
    "log_data = {\n",
    "    'val':{'loss':[], 'acc':[]},\n",
    "    'train':{'loss':[], 'acc':[]},\n",
    "    'questions':{\n",
    "        'loss':[], 'acc':[],\n",
    "        'task_acc': [], 'task_acc_cf': [], 'res_task_acc': [],\n",
    "    },\n",
    "}\n",
    "\n",
    "if not os.path.exists(SUMMARIESDIR):\n",
    "    os.makedirs(SUMMARIESDIR)\n",
    "\n",
    "train_support = get_degree_supports(adj_train, DEGREE, adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "val_support = get_degree_supports(adj_val, DEGREE, adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "test_support = get_degree_supports(adj_test, DEGREE, adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "if DATASET != 'amazon':\n",
    "    q_support = get_degree_supports(adj_q, DEGREE, adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "if DATASET == 'polyvore':\n",
    "    res_q_support = get_degree_supports(res_adj_q, DEGREE, adj_self_con=ADJ_SELF_CONNECTIONS)\n",
    "\n",
    "for i in range(1, len(train_support)):\n",
    "    train_support[i] = normalize_nonsym_adj(train_support[i])\n",
    "    val_support[i] = normalize_nonsym_adj(val_support[i])\n",
    "    test_support[i] = normalize_nonsym_adj(test_support[i])\n",
    "    if DATASET != 'amazon':\n",
    "        q_support[i] = normalize_nonsym_adj(q_support[i])\n",
    "    if DATASET == 'polyvore':\n",
    "        res_q_support[i] = normalize_nonsym_adj(res_q_support[i])    \n",
    "\n",
    "num_support = len(train_support)\n",
    "placeholders = {\n",
    "    'row_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "    'col_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'weight_decay': tf.placeholder_with_default(0., shape=()),\n",
    "    'is_train': tf.placeholder_with_default(True, shape=()),\n",
    "    'support': [tf.sparse_placeholder(tf.float32, shape=(None, None)) for sup in range(num_support)],\n",
    "    'node_features': tf.placeholder(tf.float32, shape=(None, None)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None,))   \n",
    "}\n",
    "\n",
    "model = CompatibilityGAE(placeholders,\n",
    "                    input_dim=train_features.shape[1],\n",
    "                    num_classes=NUMCLASSES,\n",
    "                    num_support=num_support,\n",
    "                    hidden=HIDDEN,\n",
    "                    learning_rate=LR,\n",
    "                    logging=True,\n",
    "                    batch_norm=BATCH_NORM,\n",
    "                    wd=args['weight_decay'])\n",
    "\n",
    "# Feed_dicts for validation and test set stay constant over different update steps\n",
    "train_feed_dict = construct_feed_dict(placeholders, train_features, train_support,\n",
    "                    train_labels, train_r_indices, train_c_indices, DO)\n",
    "if DATASET != 'amazon':\n",
    "    val_feed_dict = construct_feed_dict(placeholders, val_features, val_support,\n",
    "                        val_labels, val_r_indices, val_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "    test_feed_dict = construct_feed_dict(placeholders, test_features, test_support,\n",
    "                        test_labels, test_r_indices, test_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "    q_feed_dict = construct_feed_dict(placeholders, test_features, q_support,\n",
    "                        q_labels, q_r_indices, q_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "else:\n",
    "    val_feed_dict = construct_feed_dict(placeholders, train_features, val_support,\n",
    "                        val_labels, val_r_indices, val_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "    test_feed_dict = construct_feed_dict(placeholders, train_features, test_support,\n",
    "                        test_labels, test_r_indices, test_c_indices, 0., is_train=BN_AS_TRAIN)\n",
    "\n",
    "# Collect all variables to be logged into summary\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if WRITESUMMARY:\n",
    "    train_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/train', sess.graph)\n",
    "    val_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/val')\n",
    "else:\n",
    "    train_summary_writer = None\n",
    "    val_summary_writer = None\n",
    "\n",
    "best_val_score = 0\n",
    "best_train_score = 0\n",
    "best_epoch_train_score = 0\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "wait = 0\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "for epoch in range(NB_EPOCH):\n",
    "    t = time.time()\n",
    "\n",
    "    # modify train_feed_dict with support dropout if needed\n",
    "    if SUP_DO:\n",
    "        # do not modify the first support, the self-connections one\n",
    "        for i in range(1, len(train_support)):\n",
    "            modified = support_dropout(train_support[i].copy(), SUP_DO, edge_drop=True)\n",
    "            modified.data[...] = 1 # make it binary to normalize\n",
    "            modified = normalize_nonsym_adj(modified)\n",
    "            modified = sparse_to_tuple(modified)\n",
    "            train_feed_dict.update({placeholders['support'][i]: modified})\n",
    "\n",
    "    # run one iteration\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy, model.confmat], feed_dict=train_feed_dict)\n",
    "    \n",
    "    train_avg_loss = outs[1]\n",
    "    train_acc = outs[2]\n",
    "\n",
    "    val_avg_loss, val_acc, conf = sess.run([model.loss, model.accuracy, model.confmat], feed_dict=val_feed_dict)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"[*] Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_avg_loss),\n",
    "              \"train_acc=\", \"{:.5f}\".format(train_acc),\n",
    "              \"val_loss=\", \"{:.5f}\".format(val_avg_loss),\n",
    "              \"val_acc=\", \"{:.5f}\".format(val_acc),\n",
    "              \"\\t\\ttime=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    log_data['train']['loss'].append(float(train_avg_loss))\n",
    "    log_data['train']['acc'].append(float(train_acc))\n",
    "    log_data['val']['loss'].append(float(val_avg_loss))\n",
    "    log_data['val']['acc'].append(float(val_acc))\n",
    "\n",
    "    write_log(log_data, log_file)\n",
    "\n",
    "    if val_acc > best_val_score:\n",
    "        best_val_score = val_acc\n",
    "        best_epoch = epoch\n",
    "        best_epoch_train_score = train_acc\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"%s/best_epoch.ckpt\" % (SUMMARIESDIR))\n",
    "\n",
    "    if train_acc > best_train_score:\n",
    "        best_train_score = train_acc\n",
    "\n",
    "    if epoch % 2 == 0 and WRITESUMMARY:\n",
    "        # Train set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=train_feed_dict)\n",
    "        train_summary_writer.add_summary(summary, epoch)\n",
    "        train_summary_writer.flush()\n",
    "\n",
    "        # Validation set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=val_feed_dict)\n",
    "        val_summary_writer.add_summary(summary, epoch)\n",
    "        val_summary_writer.flush()\n",
    "\n",
    "# store model\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"%s/%s.ckpt\" % (SUMMARIESDIR, model.name), global_step=model.global_step)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"\\nOptimization Finished!\")\n",
    "    print('best validation score =', best_val_score, 'at iteration {}, with a train_score of {}'.format(best_epoch, best_epoch_train_score))\n",
    "\n",
    "print('\\nSETTINGS:\\n')\n",
    "for key, val in sorted(vars(ap.parse_args()).items()):\n",
    "    print(key, val)\n",
    "\n",
    "print('global seed = ', seed)\n",
    "\n",
    "# For parsing results from file\n",
    "results = vars(ap.parse_args()).copy()\n",
    "results.update({'best_val_score': float(best_val_score), 'best_epoch': best_epoch})\n",
    "results.update({'best_epoch_train_score': float(best_epoch_train_score)})\n",
    "results.update({'best_train_score': float(best_train_score)})\n",
    "results.update({'best_epoch': best_epoch})\n",
    "results.update({'seed':seed})\n",
    "\n",
    "print(json.dumps(results))\n",
    "\n",
    "json_outfile = SUMMARIESDIR + '/' + 'results.json'\n",
    "with open(json_outfile, 'w') as outfile:\n",
    "    json.dump(results, outfile)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
